{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision import datasets, transforms\n",
    "import torchmetrics\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For evaluation metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from torchmetrics.classification import F1Score\n",
    "\n",
    "# Set the random seeds for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If CUDA/MPS is available...\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset root directory\n",
    "root_dir = \"./Apples\"\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Set image size, modify as per dataset requirements\n",
    "image_size = (128, 128)  #\n",
    "\n",
    "dataset = datasets.ImageFolder(root=root_dir, transform=transforms.Compose([transforms.Resize(image_size), transforms.ToTensor()]))\n",
    "\n",
    "# Define dataset split sizes\n",
    "train_size = int(0.7 * len(dataset))  # 70% training\n",
    "val_size = int(0.15 * len(dataset))   # 15% validation\n",
    "test_size = len(dataset) - train_size - val_size  # Remaining for testing\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Compute mean and std for each split\n",
    "def compute_mean_std(loader):\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    total_samples = 0\n",
    "    \n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0)  # Number of images in the batch\n",
    "        images = images.view(batch_samples, 3, -1)  # Flatten H and W dimensions\n",
    "        \n",
    "        mean += images.mean(dim=[0, 2]) * batch_samples\n",
    "        std += images.std(dim=[0, 2]) * batch_samples\n",
    "        total_samples += batch_samples\n",
    "    \n",
    "    mean /= total_samples\n",
    "    std /= total_samples\n",
    "    return mean, std\n",
    "\n",
    "# Create temporary DataLoaders for mean/std calculation\n",
    "temp_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "temp_val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "temp_test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Compute statistics\n",
    "train_mean, train_std = compute_mean_std(temp_train_loader)\n",
    "val_mean, val_std = compute_mean_std(temp_val_loader)\n",
    "test_mean, test_std = compute_mean_std(temp_test_loader)\n",
    "\n",
    "# Define transforms with computed mean and std\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(train_mean.tolist(), train_std.tolist())\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(val_mean.tolist(), val_std.tolist())\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(test_mean.tolist(), test_std.tolist())\n",
    "])\n",
    "\n",
    "# Reload datasets with their respective transforms\n",
    "train_dataset.dataset.transform = train_transform\n",
    "val_dataset.dataset.transform = val_transform\n",
    "test_dataset.dataset.transform = test_transform\n",
    "\n",
    "# Create final DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model1, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(3*128*128, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.fc4 = nn.Linear(32, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc4(x) \n",
    "        return x\n",
    "\n",
    "def initialize_model():\n",
    "    return Model1()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model2, self).__init__()\n",
    "\n",
    "        # Define layers\n",
    "        self.layer1 = nn.Linear(128*128*3, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.output_layer = nn.Linear(64, 10)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.dropout(torch.relu(self.bn1(self.layer1(x)))) \n",
    "        x = self.dropout(torch.relu(self.bn2(self.layer2(x))))\n",
    "        x = self.dropout(torch.relu(self.bn3(self.layer3(x))))\n",
    "\n",
    "        return self.output_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model3, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(3*128*128, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)  # Batch normalization after first layer.\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)   # Dropout regularization with probability 0.5.\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)     # Batch normalization after second layer.\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha  \n",
    "        self.gamma = gamma  \n",
    "        self.reduction = reduction  \n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        log_probs = F.log_softmax(inputs, dim=1) \n",
    "        probs = torch.exp(log_probs) \n",
    "        ce_loss = F.nll_loss(log_probs, targets, reduction='none')\n",
    "        focal_loss = self.alpha * (1 - probs.gather(1, targets.unsqueeze(1)).squeeze()) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_misclassified(model, test_loader, device, n=36, title=\"Misclassified Examples\"):\n",
    "    model.eval()\n",
    "    misclassified_images = []\n",
    "    misclassified_preds = []\n",
    "    misclassified_targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            for i in range(len(target)):\n",
    "                if preds[i] != target[i]:\n",
    "                    misclassified_images.append(data[i].cpu().numpy().squeeze())\n",
    "                    misclassified_preds.append(preds[i].cpu().item())\n",
    "                    misclassified_targets.append(target[i].cpu().item())\n",
    "                if len(misclassified_images) >= n:\n",
    "                    break\n",
    "            if len(misclassified_images) >= n:\n",
    "                break\n",
    "    \n",
    "    # If misclassified_images has fewer than n, fill the rest with blank images\n",
    "    while len(misclassified_images) < n:\n",
    "        blank_image = np.zeros((128, 128))\n",
    "        misclassified_images.append(blank_image)\n",
    "        misclassified_preds.append(None)\n",
    "        misclassified_targets.append(None)\n",
    "    \n",
    "    # Create a 6x6 grid for 36 images; each subplot is smaller.\n",
    "    fig, axes = plt.subplots(6, 6, figsize=(8, 8))\n",
    "    fig.suptitle(title)\n",
    "    idx = 0\n",
    "    for i in range(6):\n",
    "        for j in range(6):\n",
    "            ax = axes[i, j]\n",
    "            image = misclassified_images[idx]\n",
    "            ax.imshow(image, cmap='gray')\n",
    "            if misclassified_preds[idx] is not None:\n",
    "                ax.set_title(f\"P:{misclassified_preds[idx]}\\nT:{misclassified_targets[idx]}\", fontsize=8)\n",
    "            else:\n",
    "                ax.set_title(\"Blank\", fontsize=8)\n",
    "            ax.axis('off')\n",
    "            idx += 1\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model, method=None,constant_value = 0.1): \n",
    "    if method is None:\n",
    "        print(\"No initialization required\")\n",
    "        return  \n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            if method == 'he_uniform':\n",
    "                init.kaiming_uniform_(param, nonlinearity='relu')\n",
    "            elif method == 'constant':\n",
    "                init.constant_(param, constant_value)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported initialization method: {method}\")\n",
    "        elif 'bias' in name:\n",
    "            init.constant_(param, 0.0)  # Initialize biases to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs, patience):\n",
    "    train_losses, val_losses = [], []\n",
    "    train_f1_scores, val_f1_scores = [], []\n",
    "    f1_metric = F1Score(task='multiclass', num_classes=10, average='weighted').to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    best_model_weights = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_f1 = 0.0\n",
    "\n",
    "        # Training phase\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            f1 = f1_metric(preds, targets)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_f1 += f1.item() * inputs.size(0)\n",
    "\n",
    "        # Compute epoch averages\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_f1 = running_f1 / len(train_loader.dataset)\n",
    "\n",
    "        # Append results to lists (fixed issue)\n",
    "        train_losses.append(epoch_loss)  \n",
    "        train_f1_scores.append(epoch_f1)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_f1 = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                f1 = f1_metric(preds, targets)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_f1 += f1.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_f1 = val_f1 / len(val_loader.dataset)\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        val_f1_scores.append(val_f1)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], \n",
    "              'f'Train Loss: {epoch_loss:.4f}, Train F1: {epoch_f1:.4f}, \n",
    "              'f'Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            best_model_weights = model.state_dict()\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1} (no improvement for {patience} epochs).')\n",
    "            break\n",
    "\n",
    "    if best_model_weights is not None:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "        print('Loaded the best model weights.')\n",
    "\n",
    "    return train_losses, val_losses, train_f1_scores, val_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            running_loss += loss.item() * data.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    report = classification_report(all_targets, all_preds, output_dict=True)\n",
    "    \n",
    "    return running_loss / len(test_loader.dataset), cm, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_adam(model, train_loader, val_loader, criterion, param_grid, device):\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    best_train_losses, best_val_losses = [], []\n",
    "    best_train_f1_scores, best_val_f1_scores = [], []\n",
    "    no_improve_count = 0\n",
    "    counter = 0\n",
    "    \n",
    "    for params in ParameterGrid(param_grid):\n",
    "        model = initialize_model()\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), \n",
    "            lr=params.get('lr', 0.001), \n",
    "            betas=(params.get('momentum', 0.9), 0.999), \n",
    "            weight_decay=params.get('weight_decay', 0.0)\n",
    "        )\n",
    "        counter += 1\n",
    "        print(\"Combination number\", counter)\n",
    "        \n",
    "        print(\"Current Parameters are \", params)\n",
    "        train_losses, val_losses, train_f1_scores, val_f1_scores = train_model(\n",
    "            model, train_loader, val_loader, criterion, optimizer, device, num_epochs=7, patience=3\n",
    "        )\n",
    "        \n",
    "        # Calculate average of val_f1_scores\n",
    "        avg_val_f1_score = sum(val_f1_scores) / len(val_f1_scores)\n",
    "\n",
    "        # Check if the average F1 score is higher than the best_score\n",
    "        if avg_val_f1_score > best_score:\n",
    "            best_score = avg_val_f1_score\n",
    "            print(\"New best score\", best_score)\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "            best_train_losses = train_losses\n",
    "            best_val_losses = val_losses\n",
    "            best_train_f1_scores = train_f1_scores\n",
    "            best_val_f1_scores = val_f1_scores\n",
    "            no_improve_count = 0  # Reset no_improve_count if we find a better score\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "            \n",
    "    \n",
    "    print(\"Best Parameters for Adam Optimizer:\", best_params)\n",
    "    print(\"Train Losses:\", best_train_losses)\n",
    "    print(\"Validation Losses:\", best_val_losses)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Compute y-axis limits for the first plot only\n",
    "    all_losses = best_train_losses + best_val_losses\n",
    "    y_min, y_max = min(all_losses), max(all_losses)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(best_train_losses, label='Train Loss')\n",
    "    plt.plot(best_val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.ylim(0, y_max * 1.1)  # Ensure y-axis starts from 0\n",
    "\n",
    "    # Second subplot: F1-Score Curves\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(best_train_f1_scores, label='Train F1-Score')\n",
    "    plt.plot(best_val_f1_scores, label='Validation F1-Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.title('Training and Validation F1-Score Curves')\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1)  # F1-score is typically between 0 and 1\n",
    "\n",
    "    # Show both subplots in a single figure\n",
    "    plt.tight_layout()  # Adjusts layout to prevent overlap\n",
    "    plt.show()\n",
    "    \n",
    "    return best_model, best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_RMSprop(model, train_loader, val_loader, criterion, param_grid, device):\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    best_train_losses, best_val_losses = [], []\n",
    "    best_train_f1_scores, best_val_f1_scores = [], []\n",
    "    no_improve_count = 0\n",
    "\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        model = initialize_model()\n",
    "        model.to(device)\n",
    "        \n",
    "        optimizer = torch.optim.RMSprop(\n",
    "            model.parameters(), \n",
    "            lr=params.get('lr', 0.001), \n",
    "            alpha=params.get('alpha', 0.99), \n",
    "            momentum=params.get('momentum', 0.0),  \n",
    "            weight_decay=params.get('weight_decay', 0.0),  \n",
    "        )\n",
    "        \n",
    "        train_losses, val_losses, train_f1_scores, val_f1_scores = train_model(\n",
    "            model, train_loader, val_loader, criterion, optimizer, device, num_epochs=7, patience=3\n",
    "        )\n",
    "\n",
    "        avg_val_f1_score = sum(val_f1_scores) / len(val_f1_scores)\n",
    "\n",
    "        if avg_val_f1_score > best_score:\n",
    "            best_score = avg_val_f1_score\n",
    "            print(\"New best score\", best_score)\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "            best_train_losses = train_losses\n",
    "            best_val_losses = val_losses\n",
    "            best_train_f1_scores = train_f1_scores\n",
    "            best_val_f1_scores = val_f1_scores\n",
    "            no_improve_count = 0  \n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "\n",
    "    \n",
    "        \n",
    "    print(\"Best Parameters for RMSprop optimizer:\", best_params)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    print(\"Train Losses:\", best_train_losses)\n",
    "    print(\"Validation Losses:\", best_val_losses)\n",
    "\n",
    "    # Compute y-axis limits for the first plot only\n",
    "    all_losses = best_train_losses + best_val_losses\n",
    "    y_min, y_max = min(all_losses), max(all_losses)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(best_train_losses, label='Train Loss')\n",
    "    plt.plot(best_val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.ylim(0, y_max * 1.1)  # Ensure y-axis starts from 0\n",
    "\n",
    "    # Second subplot: F1-Score Curves\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(best_train_f1_scores, label='Train F1-Score')\n",
    "    plt.plot(best_val_f1_scores, label='Validation F1-Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.title('Training and Validation F1-Score Curves')\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1)  # F1-score is typically between 0 and 1\n",
    "\n",
    "    # Show both subplots in a single figure\n",
    "    plt.tight_layout()  # Adjusts layout to prevent overlap\n",
    "    plt.show()\n",
    "    \n",
    "    return best_model, best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model 1, Adam Optimezer, Cross Entropy\n",
    "model1 = Model1().to(device)\n",
    "initialize_weights(model1, method='constant',constant_value = 0.1) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "param_grid = {'lr': [0.001, 0.0005], 'momentum': [0.95, 0.9], 'weight_decay': [ 1e-4,1e-5]}\n",
    "\n",
    "best_model_1_1, best_params_1_1, best_score_1_1 = grid_search_adam(model1, train_loader, val_loader, criterion, param_grid, device)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss_1_1, cm, report_1_1 = evaluate_model(best_model_1_1, test_loader, criterion, device)\n",
    "\n",
    "print(\"Classification Report for Model 1_1:\")\n",
    "print(report_1_1)\n",
    "\n",
    "# Plot confusion matrix for Model 1\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Model 1_1')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model 1, Adam Optimizer, Focal loss\n",
    "model1 = Model1().to(device)\n",
    "initialize_weights(model1, method='constant',constant_value = 0.1)  \n",
    "criterion = FocalLoss(alpha=1, gamma=2, reduction='mean')\n",
    "\n",
    "\n",
    "param_grid = {'lr': [0.001, 0.0005], 'momentum': [0.95, 0.9], 'weight_decay': [ 1e-4,1e-5]}\n",
    "best_model_1_2, best_params_1_2, best_score_1_2 = grid_search_adam(model1, train_loader, val_loader, criterion, param_grid, device)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss_1_2, cm, report_1_2 = evaluate_model(best_model_1_2, test_loader, criterion, device)\n",
    "\n",
    "\n",
    "print(\"Classification Report for Model 1_2:\")\n",
    "print(report_1_2)\n",
    "\n",
    "# Plot confusion matrix for Model 1\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Model 1_2')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model 1, RMS props, Cross Entropy \n",
    "model1 = Model1().to(device)\n",
    "initialize_weights(model1, method='constant',constant_value = 0.1) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'lr': [0.002, 0.0008],  \n",
    "    'alpha': [0.99, 0.95],  \n",
    "    'momentum': [0.9, 0.8],  \n",
    "    'weight_decay': [1e-4, 1e-5],  \n",
    "}\n",
    "\n",
    "best_model_1_3, best_params_1_3, best_score_1_3 = grid_search_RMSprop(model1, train_loader, val_loader, criterion, param_grid, device)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss_1_3, cm, report_1_3 = evaluate_model(best_model_1_3, test_loader, criterion, device)\n",
    "\n",
    "print(\"Classification Report for Model 1_3:\")\n",
    "print(report_1_3)\n",
    "\n",
    "# Plot confusion matrix for Model 1\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Model 1_3')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model 1, RMS props, focal loss\n",
    "model1 = Model1().to(device)\n",
    "initialize_weights(model1, method='constant',constant_value = 0.1) \n",
    "criterion = FocalLoss(alpha=1, gamma=2, reduction='mean')\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'lr': [0.002, 0.0008],  \n",
    "    'alpha': [0.99, 0.95],  \n",
    "    'momentum': [0.9, 0.8],  \n",
    "    'weight_decay': [1e-4, 1e-5],  \n",
    "}\n",
    "\n",
    "best_model_1_4, best_params_1_4, best_score_1_4 = grid_search_RMSprop(model1, train_loader, val_loader, criterion, param_grid, device)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss_1_4, cm, report_1_4 = evaluate_model(best_model_1_4, test_loader, criterion, device)\n",
    "\n",
    "print(\"Classification Report for Model 1_4:\")\n",
    "print(report_1_4)\n",
    "\n",
    "\n",
    "# Plot confusion matrix for Model 1\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Model 1_4')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scores and losses\n",
    "best_scores = {\n",
    "    'best_score_1_1': best_score_1_1,\n",
    "    'best_score_1_2': best_score_1_2,\n",
    "    'best_score_1_3': best_score_1_3,\n",
    "    'best_score_1_4': best_score_1_4\n",
    "}\n",
    "\n",
    "test_losses = {\n",
    "    'best_score_1_1': test_loss_1_1,\n",
    "    'best_score_1_2': test_loss_1_2,\n",
    "    'best_score_1_3': test_loss_1_3,\n",
    "    'best_score_1_4': test_loss_1_4\n",
    "}\n",
    "\n",
    "best_models = {\n",
    "    'best_score_1_1': best_model_1_1,\n",
    "    'best_score_1_2': best_model_1_2,\n",
    "    'best_score_1_3': best_model_1_3,\n",
    "    'best_score_1_4': best_model_1_4\n",
    "}\n",
    "\n",
    "best_params = {\n",
    "    'best_score_1_1': best_params_1_1,\n",
    "    'best_score_1_2': best_params_1_2,\n",
    "    'best_score_1_3': best_params_1_3,\n",
    "    'best_score_1_4': best_params_1_4\n",
    "}\n",
    "\n",
    "best_report = {\n",
    "    'best_score_1_1': report_1_1,\n",
    "    'best_score_1_2': report_1_2,\n",
    "    'best_score_1_3': report_1_3,\n",
    "    'best_score_1_4': report_1_4\n",
    "}\n",
    "optimizers = {\n",
    "    'best_score_1_1': \"Adam\",\n",
    "    'best_score_1_2': \"Adam\",\n",
    "    'best_score_1_3': \"RMSprop\",\n",
    "    'best_score_1_4': \"RMSprop\"\n",
    "}\n",
    "\n",
    "loss_functions = {\n",
    "    'best_score_1_1': \"Cross Entropy\",\n",
    "    'best_score_1_2': \"Focal Loss\",\n",
    "    'best_score_1_3': \"Cross Entropy\",\n",
    "    'best_score_1_4': \"Focal Loss\"\n",
    "\n",
    "}\n",
    "models = {\n",
    "    'best_score_1_1': \"Model_1_1\",\n",
    "    'best_score_1_2': \"Model_1_2\",\n",
    "    'best_score_1_3': \"Model_1_3\",\n",
    "    'best_score_1_4': \"Model_1_4\"\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "# Find the key corresponding to the best score\n",
    "best_score_key = max(best_scores, key=best_scores.get)\n",
    "\n",
    "# Assign test_loss1, best_model1, and best_params1 to the corresponding values\n",
    "test_loss1 = test_losses[best_score_key]\n",
    "best_model1 = best_models[best_score_key]\n",
    "best_params1 = best_params[best_score_key]\n",
    "best_optimizer1= optimizers[best_score_key]\n",
    "best_model_name1= models[best_score_key]\n",
    "best_loss_function1= loss_functions[best_score_key]\n",
    "best_report1 = best_report[best_score_key]\n",
    "# Print assigned values\n",
    "print(f\"Best F1 value for best Model 1: {best_score_key}\")\n",
    "print(f\"Test Loss for best Model 1: {test_loss1}\")\n",
    "print(f\"Best Model 1 is : {best_model_name1}\")\n",
    "print(f\"Best Params used on best Model 1 are: {best_params1}\")\n",
    "\n",
    "# Create a table with all model details\n",
    "data = {\n",
    "    \"Model\": list(models.values()),\n",
    "    \"Optimizer\": list(optimizers.values()),\n",
    "    \"Loss Function\": list(loss_functions.values()),\n",
    "    \"Parameters\": [str(params) for params in best_params.values()],  \n",
    "    \"F1 Score\": list(best_scores.values())\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model 2, Adam Optimizer, CrossEntropyLoss\n",
    "model2 = Model2().to(device)\n",
    "initialize_weights(model2, method='constant',constant_value = 0.1)  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {'lr': [0.001, 0.0005], 'momentum': [0.95, 0.9], 'weight_decay': [1e-4, 1e-5]}\n",
    "\n",
    "# Perform grid search using Adam optimizer\n",
    "best_model_2_1, best_params_2_1, best_score_2_1 = grid_search_adam(model2, train_loader, val_loader, criterion, param_grid, device)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss_2_1, cm, report_2_1 = evaluate_model(best_model_2_1, test_loader, criterion, device)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report for Model 2_1 :\")\n",
    "print(report_2_1)\n",
    "\n",
    "# Plot confusion matrix for Model 2\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Model 2_1')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model 2, Adam Optimizer, Focal Loss\n",
    "model2 = Model2().to(device)\n",
    "initialize_weights(model2, method='constant',constant_value = 0.1) \n",
    "criterion = FocalLoss(alpha=1, gamma=2, reduction='mean')\n",
    "\n",
    "param_grid = {'lr': [0.001, 0.0005], 'momentum': [0.95, 0.9], 'weight_decay': [1e-4, 1e-5]}\n",
    "best_model_2_2, best_params_2_2, best_score_2_2 = grid_search_adam(model2, train_loader, val_loader, criterion, param_grid, device)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss_2_2, cm, report_2_2 = evaluate_model(best_model_2_2, test_loader, criterion, device)\n",
    "\n",
    "print(\"Classification Report for Model 2_2:\")\n",
    "print(report_2_2)\n",
    "\n",
    "# Plot confusion matrix for Model 2\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Model 2_2')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model 2, RMSprop Optimizer, Cross Entropy Loss\n",
    "model2 = Model2().to(device)\n",
    "initialize_weights(model2, method='constant',constant_value = 0.1)   \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "param_grid = {\n",
    "    'lr': [0.002, 0.0008],\n",
    "    'alpha': [0.99, 0.95],\n",
    "    'momentum': [0.9, 0.8],\n",
    "    'weight_decay': [1e-4, 1e-5],\n",
    "}\n",
    "\n",
    "best_model_2_3, best_params_2_3, best_score_2_3 = grid_search_RMSprop(model2, train_loader, val_loader, criterion, param_grid, device)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss_2_3, cm, report_2_3 = evaluate_model(best_model_2_3, test_loader, criterion, device)\n",
    "\n",
    "print(\"Classification Report for Model 2_3:\")\n",
    "print(report_2_3)\n",
    "\n",
    "# Plot confusion matrix for Model 2\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Model 2_3')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model 2, RMSprop Optimizer, Focal Loss\n",
    "model2 = Model2().to(device)\n",
    "initialize_weights(model2, method='constant',constant_value = 0.1) \n",
    "criterion = FocalLoss(alpha=1, gamma=2, reduction='mean')\n",
    "\n",
    "param_grid = {\n",
    "    'lr': [0.002, 0.0008],\n",
    "    'alpha': [0.99, 0.95],\n",
    "    'momentum': [0.9, 0.8],\n",
    "    'weight_decay': [1e-4, 1e-5],\n",
    "}\n",
    "\n",
    "best_model_2_4, best_params_2_4, best_score_2_4 = grid_search_RMSprop(model2, train_loader, val_loader, criterion, param_grid, device)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss_2_4, cm, report_2_4 = evaluate_model(best_model_2_4, test_loader, criterion, device)\n",
    "\n",
    "print(\"Classification Report for Model 2_4:\")\n",
    "print(report_2_4)\n",
    "\n",
    "# Plot confusion matrix for Model 2\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Model 2_4')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scores and losses\n",
    "best_scores = {\n",
    "    'best_score_2_1': best_score_2_1,\n",
    "    'best_score_2_2': best_score_2_2,\n",
    "    'best_score_2_3': best_score_2_3,\n",
    "    'best_score_2_4': best_score_2_4\n",
    "}\n",
    "\n",
    "test_losses = {\n",
    "    'best_score_2_1': test_loss_2_1,\n",
    "    'best_score_2_2': test_loss_2_2,\n",
    "    'best_score_2_3': test_loss_2_3,\n",
    "    'best_score_2_4': test_loss_2_4\n",
    "}\n",
    "\n",
    "best_models = {\n",
    "    'best_score_2_1': best_model_2_1,\n",
    "    'best_score_2_2': best_model_2_2,\n",
    "    'best_score_2_3': best_model_2_3,\n",
    "    'best_score_2_4': best_model_2_4\n",
    "}\n",
    "\n",
    "best_params = {\n",
    "    'best_score_2_1': best_params_2_1,\n",
    "    'best_score_2_2': best_params_2_2,\n",
    "    'best_score_2_3': best_params_2_3,\n",
    "    'best_score_2_4': best_params_2_4\n",
    "}\n",
    "\n",
    "best_report = {\n",
    "    'best_score_2_1': report_2_1,\n",
    "    'best_score_2_2': report_2_2,\n",
    "    'best_score_2_3': report_2_3,\n",
    "    'best_score_2_4': report_2_4\n",
    "}\n",
    "\n",
    "optimizers = {\n",
    "    'best_score_2_1': \"Adam\",\n",
    "    'best_score_2_2': \"Adam\",\n",
    "    'best_score_2_3': \"RMSprop\",\n",
    "    'best_score_2_4': \"RMSprop\"\n",
    "}\n",
    "\n",
    "loss_functions = {\n",
    "    'best_score_2_1': \"Cross Entropy\",\n",
    "    'best_score_2_2': \"Focal Loss\",\n",
    "    'best_score_2_3': \"Cross Entropy\",\n",
    "    'best_score_2_4': \"Focal Loss\"\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'best_score_2_1': \"Model_2_1\",\n",
    "    'best_score_2_2': \"Model_2_2\",\n",
    "    'best_score_2_3': \"Model_2_3\",\n",
    "    'best_score_2_4': \"Model_2_4\"\n",
    "}\n",
    "\n",
    "# Find the key corresponding to the best score\n",
    "best_score_key = max(best_scores, key=best_scores.get)\n",
    "\n",
    "# Assign test_loss2, best_model2, and best_params2 to the corresponding values\n",
    "test_loss2 = test_losses[best_score_key]\n",
    "best_model2 = best_models[best_score_key]\n",
    "best_params2 = best_params[best_score_key]\n",
    "best_optimizer2 = optimizers[best_score_key]\n",
    "best_model_name2 = models[best_score_key]\n",
    "best_loss_function2 = loss_functions[best_score_key]\n",
    "best_report2 = best_report[best_score_key]\n",
    "\n",
    "# Print assigned values\n",
    "print(f\"Best F1 value for best Model 2: {best_score_key}\")\n",
    "print(f\"Test Loss for best Model 2: {test_loss2}\")\n",
    "print(f\"Best Model 2 is : {best_model_name2}\")\n",
    "print(f\"Best Params used on best Model 2 are: {best_params2}\")\n",
    "\n",
    "# Create a table with all model details\n",
    "data = {\n",
    "    \"Model\": list(models.values()),\n",
    "    \"Optimizer\": list(optimizers.values()),\n",
    "    \"Loss Function\": list(loss_functions.values()),\n",
    "    \"Parameters\": [str(params) for params in best_params.values()],  \n",
    "    \"F1 Score\": list(best_scores.values())\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model 3, Adam optimzer and Crossentropy\n",
    "model3 = Model3().to(device)\n",
    "initialize_weights(model3, method='constant',constant_value = 0.1)  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "param_grid = {'lr': [0.001, 0.0005], 'momentum': [0.95, 0.9], 'weight_decay': [ 1e-4,1e-5]}\n",
    "best_model_3_1, best_params_3_1, best_score_3_1 = grid_search_adam(model3, train_loader, val_loader, criterion, param_grid, device)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss_3_1, cm, report_3_1 = evaluate_model(best_model_3_1, test_loader, criterion, device)\n",
    "\n",
    "\n",
    "print(\"Classification Report for Model 3_1:\")\n",
    "print(report_3_1)\n",
    "\n",
    "# Plot confusion matrix for Model 1\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Model 3_1')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model 3, Adam optimzer and focal loss\n",
    "model3 = Model3().to(device)\n",
    "initialize_weights(model3, method='constant',constant_value = 0.1)  \n",
    "criterion = FocalLoss(alpha=1, gamma=2, reduction='mean')\n",
    "\n",
    "\n",
    "param_grid = {'lr': [0.001, 0.0005], 'momentum': [0.95, 0.9], 'weight_decay': [ 1e-4,1e-5]}\n",
    "best_model_3_2, best_params_3_2, best_score_3_2 = grid_search_adam(model3, train_loader, val_loader, criterion, param_grid, device)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss_3_2, cm, report_3_2 = evaluate_model(best_model_3_2, test_loader, criterion, device)\n",
    "\n",
    "\n",
    "print(\"Classification Report for Model 3_2:\")\n",
    "print(report_3_2)\n",
    "\n",
    "# Plot confusion matrix for Model 1\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Model 3_2')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model 3, Adam optimzer and Crossentropy\n",
    "model3 = Model3().to(device)\n",
    "initialize_weights(model3, method='constant',constant_value = 0.1)  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'lr': [0.002, 0.0008],\n",
    "    'alpha': [0.99, 0.95],\n",
    "    'momentum': [0.9, 0.8],\n",
    "    'weight_decay': [1e-4, 1e-5],\n",
    "}\n",
    "\n",
    "best_model_3_3, best_params_3_3, best_score_3_3 = grid_search_RMSprop(model3, train_loader, val_loader, criterion, param_grid, device)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss_3_3, cm, report_3_3 = evaluate_model(best_model_3_3, test_loader, criterion, device)\n",
    "\n",
    "print(\"Classification Report for Model 3_3:\")\n",
    "print(report_3_3)\n",
    "\n",
    "# Plot confusion matrix for Model 1\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Model 3_3')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model 3, RMSprop and Crossentropy\n",
    "model3 = Model3().to(device)\n",
    "initialize_weights(model3, method='constant',constant_value = 0.1)   \n",
    "criterion = FocalLoss(alpha=1, gamma=2, reduction='mean')\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'lr': [0.002, 0.0008],\n",
    "    'alpha': [0.99, 0.95],\n",
    "    'momentum': [0.9, 0.8],\n",
    "    'weight_decay': [1e-4, 1e-5],\n",
    "}\n",
    "\n",
    "best_model_3_4, best_params_3_4, best_score_3_4 = grid_search_RMSprop(model3, train_loader, val_loader, criterion, param_grid, device)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss_3_4, cm, report_3_4 = evaluate_model(best_model_3_4, test_loader, criterion, device)\n",
    "\n",
    "print(\"Classification Report for Model 3_4:\")\n",
    "print(report_3_4)\n",
    "\n",
    "# Plot confusion matrix for Model 1\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Model 3_4')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scores and losses\n",
    "best_scores = {\n",
    "    'best_score_3_1': best_score_3_1,\n",
    "    'best_score_3_2': best_score_3_2,\n",
    "    'best_score_3_3': best_score_3_3,\n",
    "    'best_score_3_4': best_score_3_4\n",
    "}\n",
    "\n",
    "test_losses = {\n",
    "    'best_score_3_1': test_loss_3_1,\n",
    "    'best_score_3_2': test_loss_3_2,\n",
    "    'best_score_3_3': test_loss_3_3,\n",
    "    'best_score_3_4': test_loss_3_4\n",
    "}\n",
    "\n",
    "best_models = {\n",
    "    'best_score_3_1': best_model_3_1,\n",
    "    'best_score_3_2': best_model_3_2,\n",
    "    'best_score_3_3': best_model_3_3,\n",
    "    'best_score_3_4': best_model_3_4\n",
    "}\n",
    "\n",
    "best_params = {\n",
    "    'best_score_3_1': best_params_3_1,\n",
    "    'best_score_3_2': best_params_3_2,\n",
    "    'best_score_3_3': best_params_3_3,\n",
    "    'best_score_3_4': best_params_3_4\n",
    "}\n",
    "\n",
    "best_report = {\n",
    "    'best_score_3_1': report_3_1,\n",
    "    'best_score_3_2': report_3_2,\n",
    "    'best_score_3_3': report_3_3,\n",
    "    'best_score_3_4': report_3_4\n",
    "}\n",
    "\n",
    "optimizers = {\n",
    "    'best_score_3_1': \"Adam\",\n",
    "    'best_score_3_2': \"Adam\",\n",
    "    'best_score_3_3': \"RMSprop\",\n",
    "    'best_score_3_4': \"RMSprop\"\n",
    "}\n",
    "\n",
    "loss_functions = {\n",
    "    'best_score_3_1': \"Cross Entropy\",\n",
    "    'best_score_3_2': \"Focal Loss\",\n",
    "    'best_score_3_3': \"Cross Entropy\",\n",
    "    'best_score_3_4': \"Focal Loss\"\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'best_score_3_1': \"Model_3_1\",\n",
    "    'best_score_3_2': \"Model_3_2\",\n",
    "    'best_score_3_3': \"Model_3_3\",\n",
    "    'best_score_3_4': \"Model_3_4\"\n",
    "}\n",
    "\n",
    "# Find the key corresponding to the best score\n",
    "best_score_key = max(best_scores, key=best_scores.get)\n",
    "\n",
    "# Assign test_loss3, best_model3, and best_params3 to the corresponding values\n",
    "test_loss3 = test_losses[best_score_key]\n",
    "best_model3 = best_models[best_score_key]\n",
    "best_params3 = best_params[best_score_key]\n",
    "best_optimizer3 = optimizers[best_score_key]\n",
    "best_model_name3 = models[best_score_key]\n",
    "best_loss_function3 = loss_functions[best_score_key]\n",
    "best_report3 = best_report[best_score_key]\n",
    "\n",
    "# Print assigned values\n",
    "print(f\"Best F1 value for best Model 3: {best_score_key}\")\n",
    "print(f\"Test Loss for best Model 3: {test_loss3}\")\n",
    "print(f\"Best Model 3 is : {best_model_name3}\")\n",
    "print(f\"Best Params used on best Model 3 are: {best_params3}\")\n",
    "\n",
    "# Create a table with all model details\n",
    "data = {\n",
    "    \"Model\": list(models.values()),\n",
    "    \"Optimizer\": list(optimizers.values()),\n",
    "    \"Loss Function\": list(loss_functions.values()),\n",
    "    \"Parameters\": [str(params) for params in best_params.values()],  \n",
    "    \"F1 Score\": list(best_scores.values())\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_metrics(report):\n",
    "    return {\n",
    "        \"Accuracy\": report['accuracy'],\n",
    "        \"Macro Precision\": report['macro avg']['precision'],\n",
    "        \"Macro Recall\": report['macro avg']['recall'],\n",
    "        \"Macro F1-Score\": report['macro avg']['f1-score'],\n",
    "        \"Weighted Precision\": report['weighted avg']['precision'],\n",
    "        \"Weighted Recall\": report['weighted avg']['recall'],\n",
    "        \"Weighted F1-Score\": report['weighted avg']['f1-score']\n",
    "    }\n",
    "\n",
    "metrics1 = extract_metrics(best_report1)\n",
    "metrics2 = extract_metrics(best_report2)\n",
    "metrics3 = extract_metrics(best_report3)\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame([\n",
    "    {\"Model Name\": \"Model1\", \"Layers\": \"3*128*128 - 256 - 32 - 10\", \"Activation\": \"ReLU\", \"Optimizer\": best_optimizer1,\"Loss function\":best_loss_function1, **metrics1},\n",
    "    {\"Model Name\": \"Model2\", \"Layers\": \"3*128*128 - 256 - 128 - 64 -10\", \"Activation\": \"ReLU\", \"Optimizer\": best_optimizer2, \"Loss function\":best_loss_function2,**metrics2},\n",
    "    {\"Model Name\": \"Model3\", \"Layers\": \"3*128*128 - 128 - 64 - 10\", \"Activation\": \"ReLU\", \"Optimizer\": best_optimizer3,\"Loss function\":best_loss_function3, **metrics3}\n",
    "])\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('model_summary.csv', index=False)\n",
    "print(\"CSV report generated: model_summary.csv\")\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
