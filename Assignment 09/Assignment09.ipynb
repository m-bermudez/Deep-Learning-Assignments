{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34782f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.0.1\n",
      "Transformers: 4.34.0\n",
      "PEFT: 0.5.0\n",
      "Datasets: 2.14.5\n",
      "BitsAndBytes: 0.41.1\n"
     ]
    }
   ],
   "source": [
    "import torch, transformers, peft, datasets\n",
    "from importlib.metadata import version\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"PEFT: {peft.__version__}\")\n",
    "print(f\"Datasets: {datasets.__version__}\")\n",
    "print(f\"BitsAndBytes: {version('bitsandbytes')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f48a2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:❌ No NVIDIA GPU detected! Training will be extremely slow on CPU.\n",
      "WARNING:__main__:Consider using a GPU-enabled environment (Google Colab, Kaggle, etc.)\n",
      "INFO:__main__:Exiting as requested.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "Exiting due to no GPU available",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Exiting due to no GPU available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm-finetune/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DataCollatorForLanguageModeling)\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# --- Check for GPU availability ---\n",
    "def check_gpu_availability():\n",
    "    \"\"\"Check for GPU availability and print details if available.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        logger.info(f\"✅ {gpu_count} NVIDIA GPU(s) detected: {gpu_name}\")\n",
    "        for i in range(gpu_count):\n",
    "            if i > 0:  # Already printed the first one\n",
    "                logger.info(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        return True\n",
    "    else:\n",
    "        logger.warning(\"❌ No NVIDIA GPU detected! Training will be extremely slow on CPU.\")\n",
    "        logger.warning(\"Consider using a GPU-enabled environment (Google Colab, Kaggle, etc.)\")\n",
    "        return False\n",
    "\n",
    "# Check if we have a GPU\n",
    "has_gpu = check_gpu_availability()\n",
    "if not has_gpu:\n",
    "    user_response = input(\"Continue without GPU? (y/n): \")\n",
    "    if user_response.lower() != 'y':\n",
    "        logger.info(\"Exiting as requested.\")\n",
    "        raise SystemExit(\"Exiting due to no GPU available\")\n",
    "    logger.warning(\"Continuing without GPU, but training will be extremely slow.\")\n",
    "\n",
    "# --- Check package versions ---\n",
    "try:\n",
    "    import importlib.metadata as importlib_metadata\n",
    "except ImportError:\n",
    "    import importlib_metadata\n",
    "\n",
    "def get_package_version(package_name):\n",
    "    \"\"\"Get package version or return 'Not installed' if not found.\"\"\"\n",
    "    try:\n",
    "        return importlib_metadata.version(package_name)\n",
    "    except importlib_metadata.PackageNotFoundError:\n",
    "        return \"Not installed\"\n",
    "\n",
    "# Print library versions\n",
    "logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "logger.info(f\"Transformers version: {get_package_version('transformers')}\")\n",
    "logger.info(f\"PEFT version: {get_package_version('peft')}\")\n",
    "logger.info(f\"Datasets version: {get_package_version('datasets')}\")\n",
    "logger.info(f\"BitsAndBytes version: {get_package_version('bitsandbytes')}\")\n",
    "logger.info(f\"Accelerate version: {get_package_version('accelerate')}\")\n",
    "\n",
    "# --- Configuration ---\n",
    "model_id = \"EleutherAI/pythia-1b-deduped\"  # Example: Choose a ~1B param model\n",
    "dataset_name = \"imdb\"  # Replace with your chosen dataset\n",
    "dataset_text_field = \"text\"  # Field containing the text\n",
    "output_dir = \"./fine_tuned_model\"\n",
    "peft_output_dir = \"./peft_adapter\"\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 1\n",
    "batch_size = 4 if torch.cuda.is_available() else 2\n",
    "learning_rate = 2e-4\n",
    "max_length = 512\n",
    "gradient_accumulation_steps = 4\n",
    "warmup_steps = 50\n",
    "logging_steps = 10\n",
    "\n",
    "# Flag to control quantization\n",
    "skip_quantization = False\n",
    "\n",
    "# --- Load Dataset ---\n",
    "try:\n",
    "    dataset = load_dataset(dataset_name, split=\"train[:5%]\")\n",
    "    logger.info(f\"Successfully loaded dataset: {dataset_name}\")\n",
    "    logger.info(f\"Dataset size: {len(dataset)} examples\")\n",
    "    logger.info(f\"Example data point: {dataset[0]}\")\n",
    "    \n",
    "    # Map numeric labels to readable text for sentiment analysis\n",
    "    label_mapping = {0: \"negative\", 1: \"positive\"}\n",
    "    \n",
    "    def format_instruction(sample):\n",
    "        \"\"\"Format the dataset into instruction-response pairs.\"\"\"\n",
    "        # Map label if it is numeric\n",
    "        label_text = sample['label']\n",
    "        if isinstance(label_text, int):\n",
    "            label_text = label_mapping.get(label_text, str(label_text))\n",
    "        return {\n",
    "            \"text\": (\n",
    "                f\"### Instruction:\\nClassify the sentiment of the following movie review.\\n\\n\"\n",
    "                f\"### Input:\\n{sample['text']}\\n\\n\"\n",
    "                f\"### Response:\\n{label_text}\"\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    dataset = dataset.map(format_instruction)\n",
    "    logger.info(f\"Formatted dataset. Example: {dataset[0]['text'][:200]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading dataset {dataset_name}: {e}\")\n",
    "    logger.error(\"Ensure the dataset exists and is accessible, or prepare your data manually.\")\n",
    "    raise RuntimeError(f\"Failed to load dataset: {e}\")\n",
    "\n",
    "# --- Load Model & Tokenizer ---\n",
    "model_loaded = False\n",
    "\n",
    "try:\n",
    "    logger.info(f\"Loading model: {model_id}\")\n",
    "    \n",
    "    # Determine the device configuration based on available hardware\n",
    "    if torch.cuda.is_available():\n",
    "        device_map = \"auto\"  # Let the library distribute across GPUs\n",
    "        torch_dtype = torch.float16  # Use half precision on GPU\n",
    "    else:\n",
    "        device_map = \"cpu\"\n",
    "        torch_dtype = torch.float32  # Use full precision on CPU\n",
    "        # Force skip quantization on CPU\n",
    "        skip_quantization = True\n",
    "        logger.warning(\"Running on CPU - forcing skip_quantization=True\")\n",
    "    \n",
    "    if not skip_quantization:\n",
    "        try:\n",
    "            import bitsandbytes as bnb\n",
    "            logger.info(f\"Using BitsAndBytes for 4-bit quantization, version: {bnb.__version__}\")\n",
    "            \n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "            )\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=device_map,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            logger.info(\"Successfully loaded model with 4-bit quantization\")\n",
    "        except Exception as bnb_error:\n",
    "            logger.warning(f\"BitsAndBytes quantization failed: {bnb_error}\")\n",
    "            logger.warning(\"Falling back to standard precision loading\")\n",
    "            skip_quantization = True\n",
    "    \n",
    "    if skip_quantization:\n",
    "        logger.info(\"Loading model without quantization\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=device_map,\n",
    "            torch_dtype=torch_dtype,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        logger.info(\"Successfully loaded model without quantization\")\n",
    "    \n",
    "    model.config.use_cache = False\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    logger.info(\"Successfully loaded model and tokenizer\")\n",
    "    model_loaded = True\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading model {model_id}: {e}\")\n",
    "    logger.error(\"Try a different model or check your internet connection.\")\n",
    "    model_loaded = False\n",
    "\n",
    "# Verify model exists and is properly loaded\n",
    "if not model_loaded or 'model' not in locals():\n",
    "    logger.error(\"Model loading failed. Cannot continue with fine-tuning.\")\n",
    "    raise RuntimeError(\"Model loading failed\")\n",
    "\n",
    "# --- Determine LoRA Target Modules ---\n",
    "if \"pythia\" in model_id.lower() or \"neox\" in model_id.lower():\n",
    "    target_modules = [\"query_key_value\"]\n",
    "elif \"llama\" in model_id.lower() or \"mistral\" in model_id.lower():\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "elif \"opt\" in model_id.lower() or \"bloom\" in model_id.lower():\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n",
    "elif \"gpt-j\" in model_id.lower():\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\"]\n",
    "elif \"phi\" in model_id.lower():\n",
    "    target_modules = [\"Wqkv\", \"out_proj\"]\n",
    "else:\n",
    "    target_modules = [\"query_key_value\"]\n",
    "    logger.warning(f\"Using default target modules for {model_id}. Consider inspecting model.named_modules() for available modules.\")\n",
    "\n",
    "# --- PEFT Configuration (LoRA) ---\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply PEFT based on whether we're using quantization or not\n",
    "try:\n",
    "    if not skip_quantization:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        logger.info(\"Prepared model for k-bit training\")\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    logger.info(\"Applied LoRA configuration to model\")\n",
    "    \n",
    "    logger.info(\"Trainable parameters:\")\n",
    "    model.print_trainable_parameters()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error configuring PEFT: {e}\")\n",
    "    raise RuntimeError(f\"Failed to configure PEFT: {e}\")\n",
    "\n",
    "# --- Prepare dataset for training ---\n",
    "logger.info(\"Preparing dataset for training...\")\n",
    "try:\n",
    "    # Prepare tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        \"\"\"Tokenize the texts and prepare for causal language modeling.\"\"\"\n",
    "        return tokenizer(\n",
    "            examples[dataset_text_field],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    # Apply tokenization to the dataset\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[col for col in dataset.column_names if col != dataset_text_field]\n",
    "    )\n",
    "    \n",
    "    # Create the labels for causal language modeling (same as input_ids)\n",
    "    def add_labels(examples):\n",
    "        examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "        return examples\n",
    "    \n",
    "    tokenized_dataset = tokenized_dataset.map(add_labels)\n",
    "    logger.info(f\"Dataset processed. Example features: {list(tokenized_dataset[0].keys())}\")\n",
    "    \n",
    "    # Create PyTorch dataset\n",
    "    class CausalLMDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, tokenized_dataset):\n",
    "            self.tokenized_dataset = tokenized_dataset\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.tokenized_dataset)\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            item = self.tokenized_dataset[idx]\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor(item[\"input_ids\"]),\n",
    "                \"attention_mask\": torch.tensor(item[\"attention_mask\"]),\n",
    "                \"labels\": torch.tensor(item[\"labels\"])\n",
    "            }\n",
    "    \n",
    "    train_dataset = CausalLMDataset(tokenized_dataset)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Created DataLoader with batch size {batch_size}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error preparing data: {e}\")\n",
    "    raise RuntimeError(f\"Failed to prepare data: {e}\")\n",
    "\n",
    "# --- Manual Training Loop ---\n",
    "logger.info(\"Setting up manual training loop...\")\n",
    "\n",
    "try:\n",
    "    # Adjust batch size based on GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3  # in GB\n",
    "        if gpu_mem < 12:  # For GPUs with less than 12GB memory\n",
    "            batch_size = 2\n",
    "            logger.info(f\"Detected GPU with {gpu_mem:.1f}GB memory. Using batch size {batch_size}\")\n",
    "    \n",
    "    # Set up optimizer with weight decay\n",
    "    from torch.optim import AdamW\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    \n",
    "    # Only optimize parameters that require gradients\n",
    "    optimizer = AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=learning_rate,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler with warmup\n",
    "    total_steps = len(train_dataloader) * num_epochs // gradient_accumulation_steps\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # For tracking loss\n",
    "    total_loss = 0\n",
    "    running_loss = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    # Make sure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    logger.info(f\"Starting training for {num_epochs} epochs\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # Use tqdm for progress display\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "        \n",
    "        # Reset gradients at the beginning of each epoch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss / gradient_accumulation_steps  # Normalize loss\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update tracking\n",
    "            running_loss += loss.item() * gradient_accumulation_steps\n",
    "            epoch_loss += loss.item() * gradient_accumulation_steps\n",
    "            step_count += 1\n",
    "            \n",
    "            # Update progress bar description\n",
    "            progress_bar.set_postfix({\"loss\": running_loss / step_count})\n",
    "            \n",
    "            # Update weights and reset gradients every gradient_accumulation_steps\n",
    "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Log every logging_steps gradient accumulation steps\n",
    "                if (batch_idx + 1) // gradient_accumulation_steps % logging_steps == 0:\n",
    "                    logger.info(\n",
    "                        f\"Epoch {epoch+1}, Step {(batch_idx + 1) // gradient_accumulation_steps}: \"\n",
    "                        f\"Loss = {running_loss / step_count:.4f}, \"\n",
    "                        f\"LR = {lr_scheduler.get_last_lr()[0]:.7f}\"\n",
    "                    )\n",
    "        \n",
    "        # Epoch complete\n",
    "        avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "        logger.info(f\"Epoch {epoch+1} complete. Average loss: {avg_epoch_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint at the end of each epoch\n",
    "        checkpoint_path = os.path.join(output_dir, f\"checkpoint-epoch-{epoch+1}\")\n",
    "        model.save_pretrained(checkpoint_path)\n",
    "        tokenizer.save_pretrained(checkpoint_path)\n",
    "        logger.info(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "    \n",
    "    logger.info(\"Training completed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in manual training loop: {e}\")\n",
    "    raise RuntimeError(f\"Training failed: {e}\")\n",
    "\n",
    "# --- Save PEFT Adapter ---\n",
    "logger.info(f\"Saving final PEFT adapter to {peft_output_dir}\")\n",
    "try:\n",
    "    model.save_pretrained(peft_output_dir)\n",
    "    tokenizer.save_pretrained(peft_output_dir)\n",
    "    logger.info(f\"Adapter weights and tokenizer saved to {peft_output_dir}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error saving adapter: {e}\")\n",
    "    raise RuntimeError(f\"Failed to save adapter: {e}\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "logger.info(\"Running evaluation on test set...\")\n",
    "try:\n",
    "    test_dataset = load_dataset(dataset_name, split=\"test[:50]\")\n",
    "    test_dataset = test_dataset.map(format_instruction)\n",
    "    \n",
    "    def evaluate_model(model, tokenizer, test_dataset, num_samples=10):\n",
    "        \"\"\"Evaluate the fine-tuned model by generating responses and comparing them to the ground truth.\"\"\"\n",
    "        model.eval()\n",
    "        results = []\n",
    "        \n",
    "        for i, sample in enumerate(test_dataset):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            input_text = sample['text'].split(\"### Response:\")[0] + \"### Response:\"\n",
    "            ground_truth = sample['text'].split(\"### Response:\")[-1].strip()\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    max_new_tokens=50,\n",
    "                    temperature=0.7,\n",
    "                    num_return_sequences=1,\n",
    "                )\n",
    "            \n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            response = generated_text.split(\"### Response:\")[-1].strip()\n",
    "            \n",
    "            # For sentiment classification tasks, use robust metric comparison.\n",
    "            if dataset_name == \"imdb\":\n",
    "                correct = ground_truth.lower() in response.lower()\n",
    "            else:\n",
    "                correct = None\n",
    "            \n",
    "            results.append({\n",
    "                \"input\": input_text,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"generated\": response,\n",
    "                \"correct\": correct\n",
    "            })\n",
    "            \n",
    "            if i < 3:\n",
    "                logger.info(f\"\\nExample {i+1}:\")\n",
    "                logger.info(f\"Input: {input_text[:100]}...\")\n",
    "                logger.info(f\"Ground truth: {ground_truth}\")\n",
    "                logger.info(f\"Generated: {response}\")\n",
    "                logger.info(f\"Correct: {correct}\")\n",
    "        \n",
    "        if all(r[\"correct\"] is not None for r in results):\n",
    "            accuracy = sum(r[\"correct\"] for r in results) / len(results)\n",
    "            logger.info(f\"\\nSample accuracy: {accuracy:.2f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    from peft import PeftModel\n",
    "    \n",
    "    # Load the model for evaluation\n",
    "    eval_device_map = \"auto\" if torch.cuda.is_available() else \"cpu\"\n",
    "    eval_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=eval_device_map,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    )\n",
    "    eval_model = PeftModel.from_pretrained(eval_model, peft_output_dir)\n",
    "    \n",
    "    results = evaluate_model(eval_model, tokenizer, test_dataset, num_samples=10)\n",
    "    logger.info(\"Evaluation complete.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during evaluation: {e}\")\n",
    "    logger.error(\"You'll need to implement proper evaluation based on your task.\")\n",
    "\n",
    "logger.info(\"\\nFine-tuning process complete!\")\n",
    "logger.info(\"Next steps:\")\n",
    "logger.info(\"1. Analyze your results and model performance.\")\n",
    "logger.info(\"2. Experiment with different hyperparameters.\")\n",
    "logger.info(\"3. Consider pushing your model adapter to the Hugging Face Hub.\")\n",
    "logger.info(\"4. Prepare your report documenting your process and findings.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
