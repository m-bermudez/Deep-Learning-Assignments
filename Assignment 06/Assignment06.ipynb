{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import wandb\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,accuracy_score\n",
    "\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# set repeatability\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)     # Set a random seed for CUDA operations.\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    \n",
    "    # Ensure deterministic behavior for CUDA operations (note: If you are not concerned with reportable reproducibility, set deterministic to false, and benchmark to true - as it can choose faster algorithms).\n",
    "    torch.backends.cudnn.deterministic = True  # Set cuDNN to deterministic mode - it will now only select algorithms that are known to be deterministic.\n",
    "    torch.backends.cudnn.benchmark = False  # Disable cuDNN benchmarking - it may select the best algorithms for the hardware, but it doesn't guarantee deterministic results.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data with column headers\n",
    "column_names = ['id', 'longitude', 'latitude', 'value']\n",
    "data = pd.read_csv('3D_spatial_network.txt', header=None, names=column_names)\n",
    "\n",
    "# If you need to save as a proper CSV\n",
    "data.to_csv('3D_spatial_network.csv', index=False)\n",
    "\n",
    "# For RNN preparation\n",
    "# Assuming you want to predict 'value' based on sequence of coordinates\n",
    "# You'll need to create sequences from your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        seq = data[i:i+sequence_length]\n",
    "        label = data[i+sequence_length][3]  # Assuming value is target\n",
    "        sequences.append(seq[:, 1:3])  # Using just coordinates as features\n",
    "        targets.append(label)\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Normalize the data first\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data[['longitude', 'latitude', 'value']])\n",
    "\n",
    "# Create sequences\n",
    "sequence_length = 10  # Adjust based on your needs\n",
    "X, y = create_sequences(scaled_data, sequence_length)\n",
    "\n",
    "# Split into train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# For RNN input shape: [samples, timesteps, features]\n",
    "# X_train shape will be (n_samples, sequence_length, 2) if using just coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the API key from the environment variable\n",
    "wandb.init(project=\"Assignment6\", entity=\"usf-magma\", config={\n",
    "    \"learning_rate\": 0.00085,\n",
    "    \"dropout_percentage\": 0.45,\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 15,\n",
    "    \"momentum\": (0.9, 0.92),\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"criterion\": \"MSELoss\",  # Changed to MSELoss for regression\n",
    "    \"input_size\": 1,\n",
    "    \"hidden_size\": 4\n",
    "})\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=4, dropout=0.45):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_size, \n",
    "                         hidden_size=hidden_size, \n",
    "                         batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        rnn_out, hidden = self.rnn(x)\n",
    "        last_time_step = rnn_out[:, -1, :]\n",
    "        last_time_step = self.dropout(last_time_step)\n",
    "        prediction = self.fc(last_time_step)\n",
    "        return prediction\n",
    "\n",
    "# Initialize wandb\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "if wandb_api_key:\n",
    "    wandb.login(key=wandb_api_key)\n",
    "else:\n",
    "    raise ValueError(\"WANDB_API_KEY environment variable not set!\")\n",
    "\n",
    "# Initialize model with wandb config\n",
    "model = SimpleRNN(\n",
    "    input_size=config.input_size,\n",
    "    hidden_size=config.hidden_size,\n",
    "    dropout=config.dropout_percentage\n",
    ")\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    betas=config.momentum,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "# Example training loop\n",
    "def train(model, dataloader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Log batch-level metrics\n",
    "            wandb.log({\n",
    "                \"batch_loss\": loss.item(),\n",
    "                \"epoch\": epoch,\n",
    "                \"batch\": batch_idx\n",
    "            })\n",
    "        \n",
    "        # Log epoch-level metrics\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        wandb.log({\n",
    "            \"epoch_loss\": avg_loss,\n",
    "            \"epoch\": epoch\n",
    "        })\n",
    "\n",
    "# Example usage (you'll need to replace with your actual data loading)\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample data loader (replace with your actual data)\n",
    "    # Normally you would create proper DataLoader with your sequences\n",
    "    input_data = torch.tensor([[[32.0], [31.0], [30.0]]])\n",
    "    target_data = torch.tensor([[29.5]])  # Example target\n",
    "    \n",
    "    # Create dummy dataset and dataloader\n",
    "    dataset = torch.utils.data.TensorDataset(input_data, target_data)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    train(model, dataloader, criterion, optimizer, config.epochs)\n",
    "    \n",
    "    # Save model and finish wandb\n",
    "    torch.save(model.state_dict(), \"rnn_model.pth\")\n",
    "    wandb.save(\"rnn_model.pth\")\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
