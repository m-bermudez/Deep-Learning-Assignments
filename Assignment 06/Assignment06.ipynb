{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import wandb\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,accuracy_score\n",
    "\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# set repeatability\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)     # Set a random seed for CUDA operations.\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    \n",
    "    # Ensure deterministic behavior for CUDA operations (note: If you are not concerned with reportable reproducibility, set deterministic to false, and benchmark to true - as it can choose faster algorithms).\n",
    "    torch.backends.cudnn.deterministic = True  # Set cuDNN to deterministic mode - it will now only select algorithms that are known to be deterministic.\n",
    "    torch.backends.cudnn.benchmark = False  # Disable cuDNN benchmarking - it may select the best algorithms for the hardware, but it doesn't guarantee deterministic results.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data with column headers\n",
    "column_names = ['id', 'longitude', 'latitude', 'value']\n",
    "data = pd.read_csv('3D_spatial_network.txt', header=None, names=column_names)\n",
    "\n",
    "# If you need to save as a proper CSV\n",
    "data.to_csv('3D_spatial_network.csv', index=False)\n",
    "\n",
    "# For RNN preparation\n",
    "# Assuming you want to predict 'value' based on sequence of coordinates\n",
    "# You'll need to create sequences from your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        seq = data[i:i+sequence_length]\n",
    "        label = data[i+sequence_length][2]  # Assuming value is target\n",
    "        sequences.append(seq[:, :2])  # Using just coordinates as features\n",
    "        targets.append(label)\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Normalize the data first\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data[['longitude', 'latitude', 'value']])\n",
    "\n",
    "# Create sequences\n",
    "sequence_length = 10  # Adjust based on your needs\n",
    "X, y = create_sequences(scaled_data, sequence_length)\n",
    "\n",
    "# Split into train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# For RNN input shape: [samples, timesteps, features]\n",
    "# X_train shape will be (n_samples, sequence_length, 2) if using just coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: bermudezm (usf-magma) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Developer\\Visual Studio Code\\Python\\Deep-Learning-Assignments\\Assignment 06\\wandb\\run-20250329_222914-4z1gz88e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/usf-magma/Assignment6/runs/4z1gz88e' target=\"_blank\">grateful-valley-1</a></strong> to <a href='https://wandb.ai/usf-magma/Assignment6' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/usf-magma/Assignment6' target=\"_blank\">https://wandb.ai/usf-magma/Assignment6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/usf-magma/Assignment6/runs/4z1gz88e' target=\"_blank\">https://wandb.ai/usf-magma/Assignment6/runs/4z1gz88e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize wandb\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "if wandb_api_key:\n",
    "    wandb.login(key=wandb_api_key)\n",
    "else:\n",
    "    raise ValueError(\"WANDB_API_KEY environment variable not set!\")\n",
    "\n",
    "# Load the API key from the environment variable\n",
    "wandb.init(project=\"Assignment6\", entity=\"usf-magma\", config={\n",
    "    \"learning_rate\": 0.00085,\n",
    "    \"dropout_percentage\": 0.45,\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 15,\n",
    "    \"momentum\": (0.9, 0.92),\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"criterion\": \"MSELoss\",  # Changed to MSELoss for regression\n",
    "    \"input_size\": 1,\n",
    "    \"hidden_size\": 4\n",
    "})\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DualRNN\n",
    "class DualRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=4, dropout=0.45):\n",
    "        super(DualRNN, self).__init__()\n",
    "        # First RNN for longitude sequences\n",
    "        self.rnn_long = nn.RNN(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=1, \n",
    "            batch_first=True\n",
    "        )\n",
    "        # Second RNN for latitude sequences\n",
    "        self.rnn_lat = nn.RNN(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=1, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Fully connected layer combines both RNN outputs\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)  # 2x hidden_size because we concatenate\n",
    "\n",
    "    def forward(self, x):\n",
    "        rnn_out, hidden = self.rnn(x)\n",
    "        last_time_step = rnn_out[:, -1, :]\n",
    "        last_time_step = self.dropout(last_time_step)\n",
    "        prediction = self.fc(last_time_step)\n",
    "        return prediction\n",
    "\n",
    "# Initialize model with wandb config\n",
    "model = DualRNN(\n",
    "    input_size=config.input_size,\n",
    "    hidden_size=config.hidden_size,\n",
    "    dropout=config.dropout_percentage\n",
    ").to(device)  # Don't forget to move model to device\n",
    "\n",
    "# Define loss and optimizer (same as before)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    betas=config.momentum,\n",
    "    weight_decay=config.weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split: 70% train, 30% temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Second split: 50% of temp (15% of total) for validation, 50% (15% of total) for test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print the sizes to verify\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(X, y, batch_size, shuffle=True):\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_tensor = torch.from_numpy(X).float()\n",
    "    y_tensor = torch.from_numpy(y).float()\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare all DataLoaders with appropriate settings\n",
    "train_loader = prepare_dataloader(X_train, y_train, \n",
    "                                batch_size=config.batch_size, \n",
    "                                shuffle=True)  # Shuffle for training\n",
    "\n",
    "val_loader = prepare_dataloader(X_val, y_val, \n",
    "                            batch_size=config.batch_size, \n",
    "                            shuffle=False)  # No shuffle for validation\n",
    "\n",
    "test_loader = prepare_dataloader(X_test, y_test, \n",
    "                            batch_size=config.batch_size, \n",
    "                            shuffle=False)  # No shuffle for testing\n",
    "\n",
    "# Verify the DataLoader shapes\n",
    "sample_train_batch = next(iter(train_loader))\n",
    "print(f\"Train batch - inputs: {sample_train_batch[0].shape}, targets: {sample_train_batch[1].shape}\")\n",
    "\n",
    "sample_val_batch = next(iter(val_loader))\n",
    "print(f\"Val batch - inputs: {sample_val_batch[0].shape}, targets: {sample_val_batch[1].shape}\")\n",
    "\n",
    "sample_test_batch = next(iter(test_loader))\n",
    "print(f\"Test batch - inputs: {sample_test_batch[0].shape}, targets: {sample_test_batch[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "Batch 100, Loss: 0.0642\n",
      "Batch 200, Loss: 0.0427\n",
      "Batch 300, Loss: 0.0342\n",
      "Batch 400, Loss: 0.0300\n",
      "Batch 500, Loss: 0.0274\n",
      "Batch 600, Loss: 0.0256\n",
      "Batch 700, Loss: 0.0243\n",
      "Batch 800, Loss: 0.0233\n",
      "Batch 900, Loss: 0.0225\n",
      "Batch 1000, Loss: 0.0219\n",
      "Batch 1100, Loss: 0.0214\n",
      "Batch 1200, Loss: 0.0211\n",
      "Batch 1300, Loss: 0.0207\n",
      "Batch 1400, Loss: 0.0205\n",
      "Batch 1500, Loss: 0.0202\n",
      "Batch 1600, Loss: 0.0200\n",
      "Batch 1700, Loss: 0.0198\n",
      "Batch 1800, Loss: 0.0197\n",
      "Batch 1900, Loss: 0.0195\n",
      "Batch 2000, Loss: 0.0194\n",
      "Batch 2100, Loss: 0.0193\n",
      "Batch 2200, Loss: 0.0192\n",
      "Batch 2300, Loss: 0.0191\n",
      "Batch 2400, Loss: 0.0190\n",
      "Batch 2500, Loss: 0.0189\n",
      "Batch 2600, Loss: 0.0188\n",
      "Batch 2700, Loss: 0.0187\n",
      "Batch 2800, Loss: 0.0186\n",
      "Batch 2900, Loss: 0.0186\n",
      "Batch 3000, Loss: 0.0185\n",
      "Batch 3100, Loss: 0.0185\n",
      "Batch 3200, Loss: 0.0184\n",
      "Batch 3300, Loss: 0.0183\n",
      "Batch 3400, Loss: 0.0183\n",
      "Batch 3500, Loss: 0.0183\n",
      "Batch 3600, Loss: 0.0182\n",
      "Batch 3700, Loss: 0.0182\n",
      "Batch 3800, Loss: 0.0181\n",
      "Batch 3900, Loss: 0.0181\n",
      "Batch 4000, Loss: 0.0181\n",
      "Batch 4100, Loss: 0.0180\n",
      "Batch 4200, Loss: 0.0180\n",
      "Batch 4300, Loss: 0.0179\n",
      "Batch 4400, Loss: 0.0179\n",
      "Batch 4500, Loss: 0.0179\n",
      "Batch 4600, Loss: 0.0178\n",
      "Batch 4700, Loss: 0.0178\n",
      "Batch 4800, Loss: 0.0178\n",
      "Batch 4900, Loss: 0.0178\n",
      "Batch 5000, Loss: 0.0178\n",
      "Batch 5100, Loss: 0.0177\n",
      "Batch 5200, Loss: 0.0177\n",
      "Batch 5300, Loss: 0.0177\n",
      "Batch 5400, Loss: 0.0177\n",
      "Train Loss: 0.0177, Train MAE: 0.1038, Train MSE: 0.0177\n",
      "Test Loss: 0.0166, Test MAE: 0.1015, Test MSE: 0.0166, R²: 0.0214\n",
      "Epoch 2/15\n",
      "----------\n",
      "Batch 100, Loss: 0.0164\n",
      "Batch 200, Loss: 0.0165\n",
      "Batch 300, Loss: 0.0166\n",
      "Batch 400, Loss: 0.0166\n",
      "Batch 500, Loss: 0.0165\n",
      "Batch 600, Loss: 0.0165\n",
      "Batch 700, Loss: 0.0166\n",
      "Batch 800, Loss: 0.0166\n",
      "Batch 900, Loss: 0.0166\n",
      "Batch 1000, Loss: 0.0166\n",
      "Batch 1100, Loss: 0.0166\n",
      "Batch 1200, Loss: 0.0166\n",
      "Batch 1300, Loss: 0.0166\n",
      "Batch 1400, Loss: 0.0166\n",
      "Batch 1500, Loss: 0.0166\n",
      "Batch 1600, Loss: 0.0167\n",
      "Batch 1700, Loss: 0.0167\n",
      "Batch 1800, Loss: 0.0167\n",
      "Batch 1900, Loss: 0.0167\n",
      "Batch 2000, Loss: 0.0167\n",
      "Batch 2100, Loss: 0.0167\n",
      "Batch 2200, Loss: 0.0167\n",
      "Batch 2300, Loss: 0.0167\n",
      "Batch 2400, Loss: 0.0167\n",
      "Batch 2500, Loss: 0.0167\n",
      "Batch 2600, Loss: 0.0167\n",
      "Batch 2700, Loss: 0.0166\n",
      "Batch 2800, Loss: 0.0166\n",
      "Batch 2900, Loss: 0.0167\n",
      "Batch 3000, Loss: 0.0166\n",
      "Batch 3100, Loss: 0.0166\n",
      "Batch 3200, Loss: 0.0166\n",
      "Batch 3300, Loss: 0.0166\n",
      "Batch 3400, Loss: 0.0166\n",
      "Batch 3500, Loss: 0.0166\n",
      "Batch 3600, Loss: 0.0166\n",
      "Batch 3700, Loss: 0.0166\n",
      "Batch 3800, Loss: 0.0166\n",
      "Batch 3900, Loss: 0.0166\n",
      "Batch 4000, Loss: 0.0166\n",
      "Batch 4100, Loss: 0.0166\n",
      "Batch 4200, Loss: 0.0166\n",
      "Batch 4300, Loss: 0.0166\n",
      "Batch 4400, Loss: 0.0166\n",
      "Batch 4500, Loss: 0.0166\n",
      "Batch 4600, Loss: 0.0166\n",
      "Batch 4700, Loss: 0.0166\n",
      "Batch 4800, Loss: 0.0166\n",
      "Batch 4900, Loss: 0.0166\n",
      "Batch 5000, Loss: 0.0166\n",
      "Batch 5100, Loss: 0.0166\n",
      "Batch 5200, Loss: 0.0166\n",
      "Batch 5300, Loss: 0.0166\n",
      "Batch 5400, Loss: 0.0166\n",
      "Train Loss: 0.0166, Train MAE: 0.1013, Train MSE: 0.0166\n",
      "Test Loss: 0.0164, Test MAE: 0.0999, Test MSE: 0.0164, R²: 0.0361\n",
      "Epoch 3/15\n",
      "----------\n",
      "Batch 100, Loss: 0.0166\n",
      "Batch 200, Loss: 0.0165\n",
      "Batch 300, Loss: 0.0165\n",
      "Batch 400, Loss: 0.0166\n",
      "Batch 500, Loss: 0.0165\n",
      "Batch 600, Loss: 0.0166\n",
      "Batch 700, Loss: 0.0165\n",
      "Batch 800, Loss: 0.0165\n",
      "Batch 900, Loss: 0.0164\n",
      "Batch 1000, Loss: 0.0164\n",
      "Batch 1100, Loss: 0.0164\n",
      "Batch 1200, Loss: 0.0164\n",
      "Batch 1300, Loss: 0.0165\n",
      "Batch 1400, Loss: 0.0165\n",
      "Batch 1500, Loss: 0.0164\n",
      "Batch 1600, Loss: 0.0164\n",
      "Batch 1700, Loss: 0.0165\n",
      "Batch 1800, Loss: 0.0165\n",
      "Batch 1900, Loss: 0.0164\n",
      "Batch 2000, Loss: 0.0165\n",
      "Batch 2100, Loss: 0.0165\n",
      "Batch 2200, Loss: 0.0165\n",
      "Batch 2300, Loss: 0.0165\n",
      "Batch 2400, Loss: 0.0165\n",
      "Batch 2500, Loss: 0.0165\n",
      "Batch 2600, Loss: 0.0165\n",
      "Batch 2700, Loss: 0.0164\n",
      "Batch 2800, Loss: 0.0164\n",
      "Batch 2900, Loss: 0.0164\n",
      "Batch 3000, Loss: 0.0165\n",
      "Batch 3100, Loss: 0.0164\n",
      "Batch 3200, Loss: 0.0164\n",
      "Batch 3300, Loss: 0.0165\n",
      "Batch 3400, Loss: 0.0165\n",
      "Batch 3500, Loss: 0.0165\n",
      "Batch 3600, Loss: 0.0165\n",
      "Batch 3700, Loss: 0.0165\n",
      "Batch 3800, Loss: 0.0165\n",
      "Batch 3900, Loss: 0.0165\n",
      "Batch 4000, Loss: 0.0165\n",
      "Batch 4100, Loss: 0.0165\n",
      "Batch 4200, Loss: 0.0165\n",
      "Batch 4300, Loss: 0.0165\n",
      "Batch 4400, Loss: 0.0165\n",
      "Batch 4500, Loss: 0.0165\n",
      "Batch 4600, Loss: 0.0165\n",
      "Batch 4700, Loss: 0.0165\n",
      "Batch 4800, Loss: 0.0165\n",
      "Batch 4900, Loss: 0.0165\n",
      "Batch 5000, Loss: 0.0165\n",
      "Batch 5100, Loss: 0.0165\n",
      "Batch 5200, Loss: 0.0164\n",
      "Batch 5300, Loss: 0.0164\n",
      "Batch 5400, Loss: 0.0164\n",
      "Train Loss: 0.0164, Train MAE: 0.1008, Train MSE: 0.0164\n",
      "Test Loss: 0.0162, Test MAE: 0.1003, Test MSE: 0.0162, R²: 0.0460\n",
      "Epoch 4/15\n",
      "----------\n",
      "Batch 100, Loss: 0.0169\n",
      "Batch 200, Loss: 0.0166\n",
      "Batch 300, Loss: 0.0163\n",
      "Batch 400, Loss: 0.0162\n",
      "Batch 500, Loss: 0.0163\n",
      "Batch 600, Loss: 0.0162\n",
      "Batch 700, Loss: 0.0163\n",
      "Batch 800, Loss: 0.0163\n",
      "Batch 900, Loss: 0.0163\n",
      "Batch 1000, Loss: 0.0163\n",
      "Batch 1100, Loss: 0.0163\n",
      "Batch 1200, Loss: 0.0163\n",
      "Batch 1300, Loss: 0.0163\n",
      "Batch 1400, Loss: 0.0163\n",
      "Batch 1500, Loss: 0.0163\n",
      "Batch 1600, Loss: 0.0163\n",
      "Batch 1700, Loss: 0.0163\n",
      "Batch 1800, Loss: 0.0163\n",
      "Batch 1900, Loss: 0.0163\n",
      "Batch 2000, Loss: 0.0163\n",
      "Batch 2100, Loss: 0.0163\n",
      "Batch 2200, Loss: 0.0163\n",
      "Batch 2300, Loss: 0.0163\n",
      "Batch 2400, Loss: 0.0163\n",
      "Batch 2500, Loss: 0.0163\n",
      "Batch 2600, Loss: 0.0164\n",
      "Batch 2700, Loss: 0.0163\n",
      "Batch 2800, Loss: 0.0163\n",
      "Batch 2900, Loss: 0.0164\n",
      "Batch 3000, Loss: 0.0164\n",
      "Batch 3100, Loss: 0.0164\n",
      "Batch 3200, Loss: 0.0163\n",
      "Batch 3300, Loss: 0.0163\n",
      "Batch 3400, Loss: 0.0163\n",
      "Batch 3500, Loss: 0.0163\n",
      "Batch 3600, Loss: 0.0163\n",
      "Batch 3700, Loss: 0.0163\n",
      "Batch 3800, Loss: 0.0163\n",
      "Batch 3900, Loss: 0.0163\n",
      "Batch 4000, Loss: 0.0163\n",
      "Batch 4100, Loss: 0.0163\n",
      "Batch 4200, Loss: 0.0163\n",
      "Batch 4300, Loss: 0.0163\n",
      "Batch 4400, Loss: 0.0163\n",
      "Batch 4500, Loss: 0.0163\n",
      "Batch 4600, Loss: 0.0163\n",
      "Batch 4700, Loss: 0.0163\n",
      "Batch 4800, Loss: 0.0163\n",
      "Batch 4900, Loss: 0.0163\n",
      "Batch 5000, Loss: 0.0163\n",
      "Batch 5100, Loss: 0.0163\n",
      "Batch 5200, Loss: 0.0163\n",
      "Batch 5300, Loss: 0.0163\n",
      "Batch 5400, Loss: 0.0163\n",
      "Train Loss: 0.0163, Train MAE: 0.1000, Train MSE: 0.0163\n",
      "Test Loss: 0.0160, Test MAE: 0.0987, Test MSE: 0.0160, R²: 0.0549\n",
      "Epoch 5/15\n",
      "----------\n",
      "Batch 100, Loss: 0.0161\n",
      "Batch 200, Loss: 0.0162\n",
      "Batch 300, Loss: 0.0158\n",
      "Batch 400, Loss: 0.0159\n",
      "Batch 500, Loss: 0.0160\n",
      "Batch 600, Loss: 0.0160\n",
      "Batch 700, Loss: 0.0160\n",
      "Batch 800, Loss: 0.0160\n",
      "Batch 900, Loss: 0.0160\n",
      "Batch 1000, Loss: 0.0161\n",
      "Batch 1100, Loss: 0.0161\n",
      "Batch 1200, Loss: 0.0161\n",
      "Batch 1300, Loss: 0.0161\n",
      "Batch 1400, Loss: 0.0162\n",
      "Batch 1500, Loss: 0.0162\n",
      "Batch 1600, Loss: 0.0162\n",
      "Batch 1700, Loss: 0.0162\n",
      "Batch 1800, Loss: 0.0162\n",
      "Batch 1900, Loss: 0.0162\n",
      "Batch 2000, Loss: 0.0162\n",
      "Batch 2100, Loss: 0.0162\n",
      "Batch 2200, Loss: 0.0162\n",
      "Batch 2300, Loss: 0.0162\n",
      "Batch 2400, Loss: 0.0162\n",
      "Batch 2500, Loss: 0.0162\n",
      "Batch 2600, Loss: 0.0162\n",
      "Batch 2700, Loss: 0.0162\n",
      "Batch 2800, Loss: 0.0162\n",
      "Batch 2900, Loss: 0.0162\n",
      "Batch 3000, Loss: 0.0162\n",
      "Batch 3100, Loss: 0.0162\n",
      "Batch 3200, Loss: 0.0162\n",
      "Batch 3300, Loss: 0.0162\n",
      "Batch 3400, Loss: 0.0162\n",
      "Batch 3500, Loss: 0.0162\n",
      "Batch 3600, Loss: 0.0162\n",
      "Batch 3700, Loss: 0.0162\n",
      "Batch 3800, Loss: 0.0162\n",
      "Batch 3900, Loss: 0.0162\n",
      "Batch 4000, Loss: 0.0162\n",
      "Batch 4100, Loss: 0.0162\n",
      "Batch 4200, Loss: 0.0162\n",
      "Batch 4300, Loss: 0.0162\n",
      "Batch 4400, Loss: 0.0162\n",
      "Batch 4500, Loss: 0.0162\n",
      "Batch 4600, Loss: 0.0162\n",
      "Batch 4700, Loss: 0.0163\n",
      "Batch 4800, Loss: 0.0163\n",
      "Batch 4900, Loss: 0.0162\n",
      "Batch 5000, Loss: 0.0162\n",
      "Batch 5100, Loss: 0.0163\n",
      "Batch 5200, Loss: 0.0163\n",
      "Batch 5300, Loss: 0.0163\n",
      "Batch 5400, Loss: 0.0163\n",
      "Train Loss: 0.0163, Train MAE: 0.0998, Train MSE: 0.0163\n",
      "Test Loss: 0.0160, Test MAE: 0.0997, Test MSE: 0.0160, R²: 0.0571\n",
      "Epoch 6/15\n",
      "----------\n",
      "Batch 100, Loss: 0.0164\n",
      "Batch 200, Loss: 0.0162\n",
      "Batch 300, Loss: 0.0159\n",
      "Batch 400, Loss: 0.0159\n",
      "Batch 500, Loss: 0.0159\n",
      "Batch 600, Loss: 0.0159\n",
      "Batch 700, Loss: 0.0161\n",
      "Batch 800, Loss: 0.0161\n",
      "Batch 900, Loss: 0.0162\n",
      "Batch 1000, Loss: 0.0162\n",
      "Batch 1100, Loss: 0.0162\n",
      "Batch 1200, Loss: 0.0162\n",
      "Batch 1300, Loss: 0.0163\n",
      "Batch 1400, Loss: 0.0162\n",
      "Batch 1500, Loss: 0.0162\n",
      "Batch 1600, Loss: 0.0162\n",
      "Batch 1700, Loss: 0.0162\n",
      "Batch 1800, Loss: 0.0162\n",
      "Batch 1900, Loss: 0.0162\n",
      "Batch 2000, Loss: 0.0162\n",
      "Batch 2100, Loss: 0.0162\n",
      "Batch 2200, Loss: 0.0162\n",
      "Batch 2300, Loss: 0.0162\n",
      "Batch 2400, Loss: 0.0162\n",
      "Batch 2500, Loss: 0.0162\n",
      "Batch 2600, Loss: 0.0162\n",
      "Batch 2700, Loss: 0.0162\n",
      "Batch 2800, Loss: 0.0162\n",
      "Batch 2900, Loss: 0.0162\n",
      "Batch 3000, Loss: 0.0162\n",
      "Batch 3100, Loss: 0.0162\n",
      "Batch 3200, Loss: 0.0162\n",
      "Batch 3300, Loss: 0.0162\n",
      "Batch 3400, Loss: 0.0162\n",
      "Batch 3500, Loss: 0.0162\n",
      "Batch 3600, Loss: 0.0162\n",
      "Batch 3700, Loss: 0.0162\n",
      "Batch 3800, Loss: 0.0162\n",
      "Batch 3900, Loss: 0.0162\n",
      "Batch 4000, Loss: 0.0162\n",
      "Batch 4100, Loss: 0.0162\n",
      "Batch 4200, Loss: 0.0162\n",
      "Batch 4300, Loss: 0.0162\n",
      "Batch 4400, Loss: 0.0162\n",
      "Batch 4500, Loss: 0.0162\n",
      "Batch 4600, Loss: 0.0162\n",
      "Batch 4700, Loss: 0.0162\n",
      "Batch 4800, Loss: 0.0162\n",
      "Batch 4900, Loss: 0.0162\n",
      "Batch 5000, Loss: 0.0162\n",
      "Batch 5100, Loss: 0.0162\n",
      "Batch 5200, Loss: 0.0162\n",
      "Batch 5300, Loss: 0.0162\n",
      "Batch 5400, Loss: 0.0162\n",
      "Train Loss: 0.0162, Train MAE: 0.0996, Train MSE: 0.0162\n",
      "Test Loss: 0.0160, Test MAE: 0.0999, Test MSE: 0.0160, R²: 0.0592\n",
      "Epoch 7/15\n",
      "----------\n",
      "Batch 100, Loss: 0.0163\n",
      "Batch 200, Loss: 0.0163\n",
      "Batch 300, Loss: 0.0161\n",
      "Batch 400, Loss: 0.0160\n",
      "Batch 500, Loss: 0.0162\n",
      "Batch 600, Loss: 0.0160\n",
      "Batch 700, Loss: 0.0161\n",
      "Batch 800, Loss: 0.0161\n",
      "Batch 900, Loss: 0.0161\n",
      "Batch 1000, Loss: 0.0161\n",
      "Batch 1100, Loss: 0.0161\n",
      "Batch 1200, Loss: 0.0161\n",
      "Batch 1300, Loss: 0.0161\n",
      "Batch 1400, Loss: 0.0162\n",
      "Batch 1500, Loss: 0.0161\n",
      "Batch 1600, Loss: 0.0161\n",
      "Batch 1700, Loss: 0.0161\n",
      "Batch 1800, Loss: 0.0162\n",
      "Batch 1900, Loss: 0.0161\n",
      "Batch 2000, Loss: 0.0161\n",
      "Batch 2100, Loss: 0.0161\n",
      "Batch 2200, Loss: 0.0161\n",
      "Batch 2300, Loss: 0.0162\n",
      "Batch 2400, Loss: 0.0162\n",
      "Batch 2500, Loss: 0.0161\n",
      "Batch 2600, Loss: 0.0161\n",
      "Batch 2700, Loss: 0.0162\n",
      "Batch 2800, Loss: 0.0162\n",
      "Batch 2900, Loss: 0.0162\n",
      "Batch 3000, Loss: 0.0161\n",
      "Batch 3100, Loss: 0.0162\n",
      "Batch 3200, Loss: 0.0162\n",
      "Batch 3300, Loss: 0.0162\n",
      "Batch 3400, Loss: 0.0162\n",
      "Batch 3500, Loss: 0.0162\n",
      "Batch 3600, Loss: 0.0162\n",
      "Batch 3700, Loss: 0.0162\n",
      "Batch 3800, Loss: 0.0162\n",
      "Batch 3900, Loss: 0.0162\n",
      "Batch 4000, Loss: 0.0162\n",
      "Batch 4100, Loss: 0.0162\n",
      "Batch 4200, Loss: 0.0162\n",
      "Batch 4300, Loss: 0.0161\n",
      "Batch 4400, Loss: 0.0161\n",
      "Batch 4500, Loss: 0.0162\n",
      "Batch 4600, Loss: 0.0162\n",
      "Batch 4700, Loss: 0.0161\n",
      "Batch 4800, Loss: 0.0162\n",
      "Batch 4900, Loss: 0.0162\n",
      "Batch 5000, Loss: 0.0162\n",
      "Batch 5100, Loss: 0.0162\n",
      "Batch 5200, Loss: 0.0162\n",
      "Batch 5300, Loss: 0.0162\n",
      "Batch 5400, Loss: 0.0162\n",
      "Train Loss: 0.0162, Train MAE: 0.0995, Train MSE: 0.0162\n",
      "Test Loss: 0.0160, Test MAE: 0.1001, Test MSE: 0.0160, R²: 0.0590\n",
      "Epoch 8/15\n",
      "----------\n",
      "Batch 100, Loss: 0.0155\n",
      "Batch 200, Loss: 0.0157\n",
      "Batch 300, Loss: 0.0158\n",
      "Batch 400, Loss: 0.0158\n",
      "Batch 500, Loss: 0.0158\n",
      "Batch 600, Loss: 0.0160\n",
      "Batch 700, Loss: 0.0160\n",
      "Batch 800, Loss: 0.0160\n",
      "Batch 900, Loss: 0.0160\n",
      "Batch 1000, Loss: 0.0161\n",
      "Batch 1100, Loss: 0.0162\n",
      "Batch 1200, Loss: 0.0162\n",
      "Batch 1300, Loss: 0.0162\n",
      "Batch 1400, Loss: 0.0161\n",
      "Batch 1500, Loss: 0.0161\n",
      "Batch 1600, Loss: 0.0161\n",
      "Batch 1700, Loss: 0.0161\n",
      "Batch 1800, Loss: 0.0161\n",
      "Batch 1900, Loss: 0.0161\n",
      "Batch 2000, Loss: 0.0161\n",
      "Batch 2100, Loss: 0.0161\n",
      "Batch 2200, Loss: 0.0161\n",
      "Batch 2300, Loss: 0.0161\n",
      "Batch 2400, Loss: 0.0161\n",
      "Batch 2500, Loss: 0.0161\n",
      "Batch 2600, Loss: 0.0161\n",
      "Batch 2700, Loss: 0.0161\n",
      "Batch 2800, Loss: 0.0161\n",
      "Batch 2900, Loss: 0.0161\n",
      "Batch 3000, Loss: 0.0161\n",
      "Batch 3100, Loss: 0.0161\n",
      "Batch 3200, Loss: 0.0161\n",
      "Batch 3300, Loss: 0.0161\n",
      "Batch 3400, Loss: 0.0161\n",
      "Batch 3500, Loss: 0.0161\n",
      "Batch 3600, Loss: 0.0161\n",
      "Batch 3700, Loss: 0.0161\n",
      "Batch 3800, Loss: 0.0161\n",
      "Batch 3900, Loss: 0.0161\n",
      "Batch 4000, Loss: 0.0161\n",
      "Batch 4100, Loss: 0.0161\n",
      "Batch 4200, Loss: 0.0161\n",
      "Batch 4300, Loss: 0.0161\n",
      "Batch 4400, Loss: 0.0161\n",
      "Batch 4500, Loss: 0.0161\n",
      "Batch 4600, Loss: 0.0161\n",
      "Batch 4700, Loss: 0.0161\n",
      "Batch 4800, Loss: 0.0161\n",
      "Batch 4900, Loss: 0.0161\n",
      "Batch 5000, Loss: 0.0161\n",
      "Batch 5100, Loss: 0.0161\n",
      "Batch 5200, Loss: 0.0161\n",
      "Batch 5300, Loss: 0.0161\n",
      "Batch 5400, Loss: 0.0161\n",
      "Train Loss: 0.0161, Train MAE: 0.0991, Train MSE: 0.0161\n",
      "Test Loss: 0.0158, Test MAE: 0.0987, Test MSE: 0.0158, R²: 0.0701\n",
      "Epoch 9/15\n",
      "----------\n",
      "Batch 100, Loss: 0.0160\n",
      "Batch 200, Loss: 0.0160\n",
      "Batch 300, Loss: 0.0159\n",
      "Batch 400, Loss: 0.0160\n",
      "Batch 500, Loss: 0.0160\n",
      "Batch 600, Loss: 0.0160\n",
      "Batch 700, Loss: 0.0159\n",
      "Batch 800, Loss: 0.0159\n",
      "Batch 900, Loss: 0.0159\n",
      "Batch 1000, Loss: 0.0160\n",
      "Batch 1100, Loss: 0.0160\n",
      "Batch 1200, Loss: 0.0160\n",
      "Batch 1300, Loss: 0.0160\n",
      "Batch 1400, Loss: 0.0160\n",
      "Batch 1500, Loss: 0.0160\n",
      "Batch 1600, Loss: 0.0160\n",
      "Batch 1700, Loss: 0.0160\n",
      "Batch 1800, Loss: 0.0160\n",
      "Batch 1900, Loss: 0.0160\n",
      "Batch 2000, Loss: 0.0160\n",
      "Batch 2100, Loss: 0.0160\n",
      "Batch 2200, Loss: 0.0160\n",
      "Batch 2300, Loss: 0.0160\n",
      "Batch 2400, Loss: 0.0160\n",
      "Batch 2500, Loss: 0.0160\n",
      "Batch 2600, Loss: 0.0160\n",
      "Batch 2700, Loss: 0.0160\n",
      "Batch 2800, Loss: 0.0160\n",
      "Batch 2900, Loss: 0.0160\n",
      "Batch 3000, Loss: 0.0160\n",
      "Batch 3100, Loss: 0.0160\n",
      "Batch 3200, Loss: 0.0160\n",
      "Batch 3300, Loss: 0.0160\n",
      "Batch 3400, Loss: 0.0160\n",
      "Batch 3500, Loss: 0.0160\n",
      "Batch 3600, Loss: 0.0160\n",
      "Batch 3700, Loss: 0.0160\n",
      "Batch 3800, Loss: 0.0160\n",
      "Batch 3900, Loss: 0.0160\n",
      "Batch 4000, Loss: 0.0160\n",
      "Batch 4100, Loss: 0.0160\n",
      "Batch 4200, Loss: 0.0160\n",
      "Batch 4300, Loss: 0.0160\n",
      "Batch 4400, Loss: 0.0160\n",
      "Batch 4500, Loss: 0.0160\n",
      "Batch 4600, Loss: 0.0160\n",
      "Batch 4700, Loss: 0.0160\n",
      "Batch 4800, Loss: 0.0160\n",
      "Batch 4900, Loss: 0.0160\n",
      "Batch 5000, Loss: 0.0160\n",
      "Batch 5100, Loss: 0.0160\n",
      "Batch 5200, Loss: 0.0160\n",
      "Batch 5300, Loss: 0.0160\n",
      "Batch 5400, Loss: 0.0160\n",
      "Train Loss: 0.0160, Train MAE: 0.0988, Train MSE: 0.0160\n",
      "Test Loss: 0.0158, Test MAE: 0.0968, Test MSE: 0.0158, R²: 0.0688\n",
      "Epoch 10/15\n",
      "----------\n",
      "Batch 100, Loss: 0.0165\n",
      "Batch 200, Loss: 0.0164\n",
      "Batch 300, Loss: 0.0162\n",
      "Batch 400, Loss: 0.0163\n",
      "Batch 500, Loss: 0.0162\n",
      "Batch 600, Loss: 0.0161\n",
      "Batch 700, Loss: 0.0160\n",
      "Batch 800, Loss: 0.0161\n",
      "Batch 900, Loss: 0.0161\n",
      "Batch 1000, Loss: 0.0160\n",
      "Batch 1100, Loss: 0.0160\n",
      "Batch 1200, Loss: 0.0160\n",
      "Batch 1300, Loss: 0.0160\n",
      "Batch 1400, Loss: 0.0160\n",
      "Batch 1500, Loss: 0.0160\n",
      "Batch 1600, Loss: 0.0160\n",
      "Batch 1700, Loss: 0.0160\n",
      "Batch 1800, Loss: 0.0160\n",
      "Batch 1900, Loss: 0.0160\n",
      "Batch 2000, Loss: 0.0160\n",
      "Batch 2100, Loss: 0.0160\n",
      "Batch 2200, Loss: 0.0160\n",
      "Batch 2300, Loss: 0.0160\n",
      "Batch 2400, Loss: 0.0160\n",
      "Batch 2500, Loss: 0.0160\n",
      "Batch 2600, Loss: 0.0160\n",
      "Batch 2700, Loss: 0.0160\n",
      "Batch 2800, Loss: 0.0160\n",
      "Batch 2900, Loss: 0.0159\n",
      "Batch 3000, Loss: 0.0160\n",
      "Batch 3100, Loss: 0.0160\n",
      "Batch 3200, Loss: 0.0160\n",
      "Batch 3300, Loss: 0.0160\n",
      "Batch 3400, Loss: 0.0160\n",
      "Batch 3500, Loss: 0.0160\n",
      "Batch 3600, Loss: 0.0160\n",
      "Batch 3700, Loss: 0.0160\n",
      "Batch 3800, Loss: 0.0160\n",
      "Batch 3900, Loss: 0.0160\n",
      "Batch 4000, Loss: 0.0160\n",
      "Batch 4100, Loss: 0.0160\n",
      "Batch 4200, Loss: 0.0160\n",
      "Batch 4300, Loss: 0.0160\n",
      "Batch 4400, Loss: 0.0160\n",
      "Batch 4500, Loss: 0.0160\n",
      "Batch 4600, Loss: 0.0160\n",
      "Batch 4700, Loss: 0.0160\n",
      "Batch 4800, Loss: 0.0160\n",
      "Batch 4900, Loss: 0.0160\n",
      "Batch 5000, Loss: 0.0160\n",
      "Batch 5100, Loss: 0.0160\n",
      "Batch 5200, Loss: 0.0160\n",
      "Batch 5300, Loss: 0.0160\n",
      "Batch 5400, Loss: 0.0160\n",
      "Train Loss: 0.0160, Train MAE: 0.0988, Train MSE: 0.0160\n",
      "Test Loss: 0.0158, Test MAE: 0.0978, Test MSE: 0.0158, R²: 0.0717\n",
      "Epoch 11/15\n",
      "----------\n",
      "Batch 100, Loss: 0.0156\n",
      "Batch 200, Loss: 0.0158\n",
      "Batch 300, Loss: 0.0158\n",
      "Batch 400, Loss: 0.0158\n",
      "Batch 500, Loss: 0.0159\n",
      "Batch 600, Loss: 0.0158\n",
      "Batch 700, Loss: 0.0159\n",
      "Batch 800, Loss: 0.0158\n",
      "Batch 900, Loss: 0.0158\n",
      "Batch 1000, Loss: 0.0159\n",
      "Batch 1100, Loss: 0.0159\n",
      "Batch 1200, Loss: 0.0158\n",
      "Batch 1300, Loss: 0.0158\n",
      "Batch 1400, Loss: 0.0158\n",
      "Batch 1500, Loss: 0.0159\n",
      "Batch 1600, Loss: 0.0159\n",
      "Batch 1700, Loss: 0.0159\n",
      "Batch 1800, Loss: 0.0159\n",
      "Batch 1900, Loss: 0.0159\n",
      "Batch 2000, Loss: 0.0159\n",
      "Batch 2100, Loss: 0.0159\n",
      "Batch 2200, Loss: 0.0159\n",
      "Batch 2300, Loss: 0.0159\n",
      "Batch 2400, Loss: 0.0159\n",
      "Batch 2500, Loss: 0.0159\n",
      "Batch 2600, Loss: 0.0159\n",
      "Batch 2700, Loss: 0.0159\n",
      "Batch 2800, Loss: 0.0159\n",
      "Batch 2900, Loss: 0.0159\n",
      "Batch 3000, Loss: 0.0159\n",
      "Batch 3100, Loss: 0.0159\n",
      "Batch 3200, Loss: 0.0159\n",
      "Batch 3300, Loss: 0.0159\n",
      "Batch 3400, Loss: 0.0159\n",
      "Batch 3500, Loss: 0.0159\n",
      "Batch 3600, Loss: 0.0159\n",
      "Batch 3700, Loss: 0.0159\n",
      "Batch 3800, Loss: 0.0159\n",
      "Batch 3900, Loss: 0.0159\n",
      "Batch 4000, Loss: 0.0159\n",
      "Batch 4100, Loss: 0.0159\n",
      "Batch 4200, Loss: 0.0159\n",
      "Batch 4300, Loss: 0.0159\n",
      "Batch 4400, Loss: 0.0159\n",
      "Batch 4500, Loss: 0.0159\n",
      "Batch 4600, Loss: 0.0159\n",
      "Batch 4700, Loss: 0.0159\n",
      "Batch 4800, Loss: 0.0159\n",
      "Batch 4900, Loss: 0.0159\n",
      "Batch 5000, Loss: 0.0159\n",
      "Batch 5100, Loss: 0.0159\n",
      "Batch 5200, Loss: 0.0159\n",
      "Batch 5300, Loss: 0.0160\n",
      "Batch 5400, Loss: 0.0159\n",
      "Train Loss: 0.0159, Train MAE: 0.0986, Train MSE: 0.0159\n",
      "Test Loss: 0.0157, Test MAE: 0.0965, Test MSE: 0.0157, R²: 0.0745\n",
      "Epoch 12/15\n",
      "----------\n",
      "Batch 100, Loss: 0.0160\n",
      "Batch 200, Loss: 0.0160\n",
      "Batch 300, Loss: 0.0160\n",
      "Batch 400, Loss: 0.0160\n",
      "Batch 500, Loss: 0.0158\n",
      "Batch 600, Loss: 0.0158\n",
      "Batch 700, Loss: 0.0159\n",
      "Batch 800, Loss: 0.0158\n",
      "Batch 900, Loss: 0.0159\n",
      "Batch 1000, Loss: 0.0159\n",
      "Batch 1100, Loss: 0.0159\n",
      "Batch 1200, Loss: 0.0159\n",
      "Batch 1300, Loss: 0.0159\n",
      "Batch 1400, Loss: 0.0159\n",
      "Batch 1500, Loss: 0.0159\n",
      "Batch 1600, Loss: 0.0159\n",
      "Batch 1700, Loss: 0.0159\n",
      "Batch 1800, Loss: 0.0159\n",
      "Batch 1900, Loss: 0.0159\n",
      "Batch 2000, Loss: 0.0159\n",
      "Batch 2100, Loss: 0.0159\n",
      "Batch 2200, Loss: 0.0159\n",
      "Batch 2300, Loss: 0.0159\n",
      "Batch 2400, Loss: 0.0159\n",
      "Batch 2500, Loss: 0.0159\n",
      "Batch 2600, Loss: 0.0159\n",
      "Batch 2700, Loss: 0.0160\n",
      "Batch 2800, Loss: 0.0160\n",
      "Batch 2900, Loss: 0.0160\n",
      "Batch 3000, Loss: 0.0160\n",
      "Batch 3100, Loss: 0.0160\n",
      "Batch 3200, Loss: 0.0160\n",
      "Batch 3300, Loss: 0.0160\n",
      "Batch 3400, Loss: 0.0160\n",
      "Batch 3500, Loss: 0.0160\n",
      "Batch 3600, Loss: 0.0160\n",
      "Batch 3700, Loss: 0.0160\n",
      "Batch 3800, Loss: 0.0160\n",
      "Batch 3900, Loss: 0.0160\n",
      "Batch 4000, Loss: 0.0160\n",
      "Batch 4100, Loss: 0.0160\n",
      "Batch 4200, Loss: 0.0160\n",
      "Batch 4300, Loss: 0.0160\n",
      "Batch 4400, Loss: 0.0160\n",
      "Batch 4500, Loss: 0.0160\n",
      "Batch 4600, Loss: 0.0160\n",
      "Batch 4700, Loss: 0.0160\n",
      "Batch 4800, Loss: 0.0160\n",
      "Batch 4900, Loss: 0.0160\n",
      "Batch 5000, Loss: 0.0160\n",
      "Batch 5100, Loss: 0.0160\n",
      "Batch 5200, Loss: 0.0160\n",
      "Batch 5300, Loss: 0.0160\n",
      "Batch 5400, Loss: 0.0160\n",
      "Train Loss: 0.0160, Train MAE: 0.0986, Train MSE: 0.0160\n",
      "Test Loss: 0.0158, Test MAE: 0.0969, Test MSE: 0.0158, R²: 0.0707\n",
      "Epoch 13/15\n",
      "----------\n",
      "Batch 100, Loss: 0.0160\n",
      "Batch 200, Loss: 0.0160\n",
      "Batch 300, Loss: 0.0159\n",
      "Batch 400, Loss: 0.0158\n",
      "Batch 500, Loss: 0.0159\n",
      "Batch 600, Loss: 0.0159\n",
      "Batch 700, Loss: 0.0160\n",
      "Batch 800, Loss: 0.0161\n",
      "Batch 900, Loss: 0.0160\n",
      "Batch 1000, Loss: 0.0159\n",
      "Batch 1100, Loss: 0.0160\n",
      "Batch 1200, Loss: 0.0160\n",
      "Batch 1300, Loss: 0.0159\n",
      "Batch 1400, Loss: 0.0159\n",
      "Batch 1500, Loss: 0.0159\n",
      "Batch 1600, Loss: 0.0160\n",
      "Batch 1700, Loss: 0.0159\n",
      "Batch 1800, Loss: 0.0159\n",
      "Batch 1900, Loss: 0.0160\n",
      "Batch 2000, Loss: 0.0160\n",
      "Batch 2100, Loss: 0.0160\n",
      "Batch 2200, Loss: 0.0160\n",
      "Batch 2300, Loss: 0.0160\n",
      "Batch 2400, Loss: 0.0160\n",
      "Batch 2500, Loss: 0.0160\n",
      "Batch 2600, Loss: 0.0160\n",
      "Batch 2700, Loss: 0.0160\n",
      "Batch 2800, Loss: 0.0159\n",
      "Batch 2900, Loss: 0.0159\n",
      "Batch 3000, Loss: 0.0160\n",
      "Batch 3100, Loss: 0.0159\n",
      "Batch 3200, Loss: 0.0160\n",
      "Batch 3300, Loss: 0.0159\n",
      "Batch 3400, Loss: 0.0159\n",
      "Batch 3500, Loss: 0.0160\n",
      "Batch 3600, Loss: 0.0159\n",
      "Batch 3700, Loss: 0.0159\n",
      "Batch 3800, Loss: 0.0160\n",
      "Batch 3900, Loss: 0.0160\n",
      "Batch 4000, Loss: 0.0160\n",
      "Batch 4100, Loss: 0.0160\n",
      "Batch 4200, Loss: 0.0159\n",
      "Batch 4300, Loss: 0.0160\n",
      "Batch 4400, Loss: 0.0160\n",
      "Batch 4500, Loss: 0.0160\n",
      "Batch 4600, Loss: 0.0160\n",
      "Batch 4700, Loss: 0.0160\n",
      "Batch 4800, Loss: 0.0160\n",
      "Batch 4900, Loss: 0.0160\n",
      "Batch 5000, Loss: 0.0160\n",
      "Batch 5100, Loss: 0.0160\n",
      "Batch 5200, Loss: 0.0160\n",
      "Batch 5300, Loss: 0.0160\n",
      "Batch 5400, Loss: 0.0160\n",
      "Train Loss: 0.0159, Train MAE: 0.0986, Train MSE: 0.0159\n",
      "Test Loss: 0.0158, Test MAE: 0.0988, Test MSE: 0.0158, R²: 0.0708\n",
      "Epoch 14/15\n",
      "----------\n",
      "Batch 100, Loss: 0.0155\n",
      "Batch 200, Loss: 0.0157\n",
      "Batch 300, Loss: 0.0157\n",
      "Batch 400, Loss: 0.0158\n",
      "Batch 500, Loss: 0.0158\n",
      "Batch 600, Loss: 0.0158\n",
      "Batch 700, Loss: 0.0157\n",
      "Batch 800, Loss: 0.0158\n",
      "Batch 900, Loss: 0.0157\n",
      "Batch 1000, Loss: 0.0158\n",
      "Batch 1100, Loss: 0.0158\n",
      "Batch 1200, Loss: 0.0159\n",
      "Batch 1300, Loss: 0.0159\n",
      "Batch 1400, Loss: 0.0160\n",
      "Batch 1500, Loss: 0.0160\n",
      "Batch 1600, Loss: 0.0160\n",
      "Batch 1700, Loss: 0.0160\n",
      "Batch 1800, Loss: 0.0160\n",
      "Batch 1900, Loss: 0.0160\n",
      "Batch 2000, Loss: 0.0159\n",
      "Batch 2100, Loss: 0.0159\n",
      "Batch 2200, Loss: 0.0159\n",
      "Batch 2300, Loss: 0.0159\n",
      "Batch 2400, Loss: 0.0159\n",
      "Batch 2500, Loss: 0.0159\n",
      "Batch 2600, Loss: 0.0159\n",
      "Batch 2700, Loss: 0.0159\n",
      "Batch 2800, Loss: 0.0159\n",
      "Batch 2900, Loss: 0.0159\n",
      "Batch 3000, Loss: 0.0159\n",
      "Batch 3100, Loss: 0.0159\n",
      "Batch 3200, Loss: 0.0159\n",
      "Batch 3300, Loss: 0.0159\n",
      "Batch 3400, Loss: 0.0159\n",
      "Batch 3500, Loss: 0.0159\n",
      "Batch 3600, Loss: 0.0159\n",
      "Batch 3700, Loss: 0.0159\n",
      "Batch 3800, Loss: 0.0159\n",
      "Batch 3900, Loss: 0.0159\n",
      "Batch 4000, Loss: 0.0159\n",
      "Batch 4100, Loss: 0.0159\n",
      "Batch 4200, Loss: 0.0159\n",
      "Batch 4300, Loss: 0.0159\n",
      "Batch 4400, Loss: 0.0159\n",
      "Batch 4500, Loss: 0.0159\n",
      "Batch 4600, Loss: 0.0159\n",
      "Batch 4700, Loss: 0.0159\n",
      "Batch 4800, Loss: 0.0159\n",
      "Batch 4900, Loss: 0.0159\n",
      "Batch 5000, Loss: 0.0159\n",
      "Batch 5100, Loss: 0.0159\n",
      "Batch 5200, Loss: 0.0159\n",
      "Batch 5300, Loss: 0.0159\n",
      "Batch 5400, Loss: 0.0159\n",
      "Train Loss: 0.0159, Train MAE: 0.0985, Train MSE: 0.0159\n",
      "Test Loss: 0.0157, Test MAE: 0.0978, Test MSE: 0.0157, R²: 0.0753\n",
      "Epoch 15/15\n",
      "----------\n",
      "Batch 100, Loss: 0.0158\n",
      "Batch 200, Loss: 0.0159\n",
      "Batch 300, Loss: 0.0157\n",
      "Batch 400, Loss: 0.0159\n",
      "Batch 500, Loss: 0.0157\n",
      "Batch 600, Loss: 0.0158\n",
      "Batch 700, Loss: 0.0160\n",
      "Batch 800, Loss: 0.0160\n",
      "Batch 900, Loss: 0.0159\n",
      "Batch 1000, Loss: 0.0159\n",
      "Batch 1100, Loss: 0.0159\n",
      "Batch 1200, Loss: 0.0159\n",
      "Batch 1300, Loss: 0.0160\n",
      "Batch 1400, Loss: 0.0159\n",
      "Batch 1500, Loss: 0.0159\n",
      "Batch 1600, Loss: 0.0159\n",
      "Batch 1700, Loss: 0.0159\n",
      "Batch 1800, Loss: 0.0159\n",
      "Batch 1900, Loss: 0.0159\n",
      "Batch 2000, Loss: 0.0159\n",
      "Batch 2100, Loss: 0.0159\n",
      "Batch 2200, Loss: 0.0158\n",
      "Batch 2300, Loss: 0.0158\n",
      "Batch 2400, Loss: 0.0158\n",
      "Batch 2500, Loss: 0.0159\n",
      "Batch 2600, Loss: 0.0159\n",
      "Batch 2700, Loss: 0.0159\n",
      "Batch 2800, Loss: 0.0159\n",
      "Batch 2900, Loss: 0.0159\n",
      "Batch 3000, Loss: 0.0159\n",
      "Batch 3100, Loss: 0.0159\n",
      "Batch 3200, Loss: 0.0159\n",
      "Batch 3300, Loss: 0.0159\n",
      "Batch 3400, Loss: 0.0159\n",
      "Batch 3500, Loss: 0.0159\n",
      "Batch 3600, Loss: 0.0159\n",
      "Batch 3700, Loss: 0.0159\n",
      "Batch 3800, Loss: 0.0159\n",
      "Batch 3900, Loss: 0.0159\n",
      "Batch 4000, Loss: 0.0159\n",
      "Batch 4100, Loss: 0.0159\n",
      "Batch 4200, Loss: 0.0159\n",
      "Batch 4300, Loss: 0.0159\n",
      "Batch 4400, Loss: 0.0159\n",
      "Batch 4500, Loss: 0.0159\n",
      "Batch 4600, Loss: 0.0159\n",
      "Batch 4700, Loss: 0.0159\n",
      "Batch 4800, Loss: 0.0159\n",
      "Batch 4900, Loss: 0.0159\n",
      "Batch 5000, Loss: 0.0159\n",
      "Batch 5100, Loss: 0.0159\n",
      "Batch 5200, Loss: 0.0160\n",
      "Batch 5300, Loss: 0.0160\n",
      "Batch 5400, Loss: 0.0160\n",
      "Train Loss: 0.0160, Train MAE: 0.0986, Train MSE: 0.0160\n",
      "Test Loss: 0.0157, Test MAE: 0.0973, Test MSE: 0.0157, R²: 0.0755\n",
      "Best test loss: 0.0157\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇████</td></tr><tr><td>test_loss</td><td>█▆▅▄▃▃▃▂▂▁▁▂▂▁▁</td></tr><tr><td>test_mae</td><td>█▆▆▄▅▆▆▄▁▃▁▂▄▃▂</td></tr><tr><td>test_mse</td><td>█▆▅▄▃▃▃▂▂▁▁▂▂▁▁</td></tr><tr><td>test_r2</td><td>▁▃▄▅▆▆▆▇▇██▇▇██</td></tr><tr><td>train_batch_loss</td><td>██▇▇▅▃▃▃▃▂▁▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train_mae</td><td>█▅▄▃▃▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train_mse</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>15</td></tr><tr><td>best_loss</td><td>0.0157</td></tr><tr><td>best_test_mae</td><td>0.09731</td></tr><tr><td>best_test_r2</td><td>0.07549</td></tr><tr><td>epoch</td><td>15</td></tr><tr><td>test_loss</td><td>0.0157</td></tr><tr><td>test_mae</td><td>0.09731</td></tr><tr><td>test_mse</td><td>0.0157</td></tr><tr><td>test_r2</td><td>0.07549</td></tr><tr><td>train_batch_loss</td><td>0.01596</td></tr><tr><td>train_loss</td><td>0.01596</td></tr><tr><td>train_mae</td><td>0.09858</td></tr><tr><td>train_mse</td><td>0.01596</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grateful-valley-1</strong> at: <a href='https://wandb.ai/usf-magma/Assignment6/runs/4z1gz88e' target=\"_blank\">https://wandb.ai/usf-magma/Assignment6/runs/4z1gz88e</a><br> View project at: <a href='https://wandb.ai/usf-magma/Assignment6' target=\"_blank\">https://wandb.ai/usf-magma/Assignment6</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 12 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250329_222914-4z1gz88e\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train Test Val Split\n",
    "def train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, num_epochs=config.epochs):\n",
    "    # Track best performance\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_train_preds = []\n",
    "        all_train_targets = []\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Move data to device\n",
    "            data = data.to(device).float()  # shape: (batch, seq_len, 2)\n",
    "            target = target.to(device).float().unsqueeze(1)  # shape: (batch, 1)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            # Store predictions and targets for metrics\n",
    "            all_train_preds.extend(outputs.detach().cpu().numpy())\n",
    "            all_train_targets.extend(target.detach().cpu().numpy())\n",
    "            \n",
    "            # Log batch statistics (every 100 batches)\n",
    "            if batch_idx % 100 == 99:\n",
    "            # Calculate batch loss (MSE is already your loss function)\n",
    "                batch_loss = running_loss / ((batch_idx + 1) * train_loader.batch_size)\n",
    "    \n",
    "                # Calculate batch MSE (since your loss is MSE, this is the same as batch_loss)\n",
    "                batch_mse = batch_loss  # Because you're using MSELoss\n",
    "    \n",
    "                # Calculate batch MAE (Mean Absolute Error) as an additional metric\n",
    "                batch_mae = np.mean(np.abs(np.array(all_train_preds[-100*train_loader.batch_size:]) - np.array(all_train_targets[-100*train_loader.batch_size:])))\n",
    "                print(f'Batch {batch_idx+1}, Loss (MSE): {batch_loss:.4f}, MAE: {batch_mae:.4f}')\n",
    "                wandb.log({\n",
    "                    \"train_batch_loss\": batch_loss,\n",
    "                    \"train_batch_mse\": batch_mse,  # Same as loss in this case\n",
    "                    \"train_batch_mae\": batch_mae,\n",
    "                    \"epoch\": epoch + batch_idx/len(train_loader)\n",
    "                })\n",
    "        \n",
    "        # Calculate epoch statistics for training\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_mae = np.mean(np.abs(np.array(all_train_preds) - np.array(all_train_targets)))\n",
    "        train_mse = np.mean((np.array(all_train_preds) - np.array(all_train_targets))**2)\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Train MAE: {train_mae:.4f}, Train MSE: {train_mse:.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_val_preds = []\n",
    "        all_val_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data = data.to(device).float()\n",
    "                target = target.to(device).float().unsqueeze(1)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, target)\n",
    "                \n",
    "                # Statistics\n",
    "                val_loss += loss.item() * data.size(0)\n",
    "                \n",
    "                # Store predictions and targets\n",
    "                all_val_preds.extend(outputs.cpu().numpy())\n",
    "                all_val_targets.extend(target.cpu().numpy())\n",
    "        \n",
    "        # Calculate validation statistics\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_mae = np.mean(np.abs(np.array(all_val_preds) - np.array(all_val_targets)))\n",
    "        val_mse = np.mean((np.array(all_val_preds) - np.array(all_val_targets))**2)\n",
    "        val_r2 = 1 - (val_mse / np.var(all_val_targets))\n",
    "        \n",
    "        print(f'Validation Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}, Val MSE: {val_mse:.4f}, R²: {val_r2:.4f}')\n",
    "        \n",
    "        # Test phase (only at the end of training or periodically as needed)\n",
    "        if epoch == num_epochs - 1 or epoch % 5 == 0:  # Test every 5 epochs and at the end\n",
    "            test_loss = 0.0\n",
    "            all_test_preds = []\n",
    "            all_test_targets = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for data, target in test_loader:\n",
    "                    data = data.to(device).float()\n",
    "                    target = target.to(device).float().unsqueeze(1)\n",
    "                    \n",
    "                    outputs = model(data)\n",
    "                    loss = criterion(outputs, target)\n",
    "                    \n",
    "                    test_loss += loss.item() * data.size(0)\n",
    "                    all_test_preds.extend(outputs.cpu().numpy())\n",
    "                    all_test_targets.extend(target.cpu().numpy())\n",
    "            \n",
    "            test_loss = test_loss / len(test_loader.dataset)\n",
    "            test_mae = np.mean(np.abs(np.array(all_test_preds) - np.array(all_test_targets)))\n",
    "            test_mse = np.mean((np.array(all_test_preds) - np.array(all_test_targets))**2)\n",
    "            test_r2 = 1 - (test_mse / np.var(all_test_targets))\n",
    "            \n",
    "            print(f'Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}, Test MSE: {test_mse:.4f}, R²: {test_r2:.4f}')\n",
    "        else:\n",
    "            test_loss = None\n",
    "        \n",
    "        # Log epoch statistics to W&B\n",
    "        log_dict = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_mae\": train_mae,\n",
    "            \"train_mse\": train_mse,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_mae\": val_mae,\n",
    "            \"val_mse\": val_mse,\n",
    "            \"val_r2\": val_r2,\n",
    "        }\n",
    "        \n",
    "        if test_loss is not None:\n",
    "            log_dict.update({\n",
    "                \"test_loss\": test_loss,\n",
    "                \"test_mae\": test_mae,\n",
    "                \"test_mse\": test_mse,\n",
    "                \"test_r2\": test_r2,\n",
    "            })\n",
    "        \n",
    "        wandb.log(log_dict)\n",
    "        \n",
    "        # Save model if it's the best so far (based on validation loss)\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            model_save_path = os.path.join(save_dir, f'dual_rnn_best_epoch_{epoch+1}.pth')\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            wandb.save(model_save_path)\n",
    "            \n",
    "            # Log best model metrics to W&B summary\n",
    "            wandb.run.summary[\"best_val_loss\"] = best_loss\n",
    "            wandb.run.summary[\"best_epoch\"] = epoch + 1\n",
    "            wandb.run.summary[\"best_val_mae\"] = val_mae\n",
    "            wandb.run.summary[\"best_val_r2\"] = val_r2\n",
    "    \n",
    "    print(f'Best validation loss: {best_loss:.4f}')\n",
    "    return model\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a folder for saving models if it doesn't exist\n",
    "    save_dir = \"saved_models\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Prepare data loaders\n",
    "    train_loader = prepare_dataloader(X_train, y_train, config.batch_size)\n",
    "    val_loader = prepare_dataloader(X_val, y_val, config.batch_size)\n",
    "    test_loader = prepare_dataloader(X_test, y_test, config.batch_size)\n",
    "    \n",
    "    # Train the model\n",
    "    model = train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, config.epochs)\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = os.path.join(save_dir, \"dual_rnn_final.pth\")\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    wandb.save(final_model_path)\n",
    "    \n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
